{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feraguilari/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/feraguilari/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Yyour code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here; load and preview the dataset\n",
    "data = pd.read_csv('Bank_complaints.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "import random\n",
    "random.seed(123)\n",
    "data = data.sample(10000)\n",
    "data.index = range(10000)\n",
    "product = data['Product']\n",
    "complaints = data[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "oh_results = tokenizer.texts_to_matrix(complaints,mode='binary')\n",
    "word_index= tokenizer.word_index\n",
    "oh_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "from keras.utils.np_utils import to_categorical\n",
    "product_oh = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(oh_results, product_oh, \n",
    "                                                    test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(50,activation='relu', input_shape=(2000,)))\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "model.compile(optimizer = 'SGD',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/feraguilari/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9560 - acc: 0.1623 - val_loss: 1.9359 - val_acc: 0.1840\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9243 - acc: 0.2093 - val_loss: 1.9107 - val_acc: 0.2390\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8988 - acc: 0.2425 - val_loss: 1.8872 - val_acc: 0.2560\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8736 - acc: 0.2647 - val_loss: 1.8623 - val_acc: 0.2750\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8462 - acc: 0.2813 - val_loss: 1.8340 - val_acc: 0.3070\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8142 - acc: 0.2995 - val_loss: 1.8007 - val_acc: 0.3340\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7778 - acc: 0.3196 - val_loss: 1.7631 - val_acc: 0.3570\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7375 - acc: 0.3423 - val_loss: 1.7217 - val_acc: 0.3660\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6954 - acc: 0.3572 - val_loss: 1.6799 - val_acc: 0.3770\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6520 - acc: 0.3732 - val_loss: 1.6373 - val_acc: 0.3950\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6083 - acc: 0.3900 - val_loss: 1.5949 - val_acc: 0.4080\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5649 - acc: 0.4060 - val_loss: 1.5523 - val_acc: 0.4200\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5212 - acc: 0.4251 - val_loss: 1.5124 - val_acc: 0.4510\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4779 - acc: 0.4512 - val_loss: 1.4676 - val_acc: 0.4640\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4338 - acc: 0.4781 - val_loss: 1.4262 - val_acc: 0.4950\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3894 - acc: 0.5064 - val_loss: 1.3835 - val_acc: 0.5180\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3441 - acc: 0.5335 - val_loss: 1.3424 - val_acc: 0.5560\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2992 - acc: 0.5621 - val_loss: 1.2992 - val_acc: 0.5780\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2547 - acc: 0.5867 - val_loss: 1.2592 - val_acc: 0.5980\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2120 - acc: 0.6095 - val_loss: 1.2190 - val_acc: 0.6100\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1714 - acc: 0.6229 - val_loss: 1.1833 - val_acc: 0.6380\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1321 - acc: 0.6412 - val_loss: 1.1447 - val_acc: 0.6400\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0948 - acc: 0.6515 - val_loss: 1.1108 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0602 - acc: 0.6621 - val_loss: 1.0790 - val_acc: 0.6600\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0278 - acc: 0.6699 - val_loss: 1.0498 - val_acc: 0.6650\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9973 - acc: 0.6804 - val_loss: 1.0217 - val_acc: 0.6710\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9690 - acc: 0.6837 - val_loss: 0.9966 - val_acc: 0.6810\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9426 - acc: 0.6920 - val_loss: 0.9734 - val_acc: 0.6840\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9181 - acc: 0.7011 - val_loss: 0.9509 - val_acc: 0.6910\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8955 - acc: 0.7071 - val_loss: 0.9318 - val_acc: 0.6850\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8741 - acc: 0.7165 - val_loss: 0.9133 - val_acc: 0.6880\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8546 - acc: 0.7232 - val_loss: 0.8942 - val_acc: 0.6960\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8356 - acc: 0.7253 - val_loss: 0.8787 - val_acc: 0.7000\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8187 - acc: 0.7300 - val_loss: 0.8632 - val_acc: 0.7000\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8022 - acc: 0.7339 - val_loss: 0.8504 - val_acc: 0.7060\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7871 - acc: 0.7377 - val_loss: 0.8391 - val_acc: 0.7030\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7728 - acc: 0.7415 - val_loss: 0.8246 - val_acc: 0.7120\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7595 - acc: 0.7456 - val_loss: 0.8142 - val_acc: 0.7160\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7466 - acc: 0.7497 - val_loss: 0.8074 - val_acc: 0.7110\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7347 - acc: 0.7544 - val_loss: 0.7939 - val_acc: 0.7180\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7233 - acc: 0.7551 - val_loss: 0.7822 - val_acc: 0.7260\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7123 - acc: 0.7632 - val_loss: 0.7740 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7018 - acc: 0.7636 - val_loss: 0.7685 - val_acc: 0.7300\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6922 - acc: 0.7672 - val_loss: 0.7589 - val_acc: 0.7320\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6823 - acc: 0.7677 - val_loss: 0.7529 - val_acc: 0.7310\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6730 - acc: 0.7724 - val_loss: 0.7461 - val_acc: 0.7400\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6647 - acc: 0.7744 - val_loss: 0.7399 - val_acc: 0.7370\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6559 - acc: 0.7771 - val_loss: 0.7326 - val_acc: 0.7490\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6482 - acc: 0.7767 - val_loss: 0.7291 - val_acc: 0.7450\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6406 - acc: 0.7811 - val_loss: 0.7214 - val_acc: 0.7470\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6328 - acc: 0.7847 - val_loss: 0.7159 - val_acc: 0.7450\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6260 - acc: 0.7871 - val_loss: 0.7103 - val_acc: 0.7560\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6188 - acc: 0.7881 - val_loss: 0.7061 - val_acc: 0.7520\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6120 - acc: 0.7920 - val_loss: 0.7013 - val_acc: 0.7540\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6050 - acc: 0.7955 - val_loss: 0.6995 - val_acc: 0.7470\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5992 - acc: 0.7953 - val_loss: 0.6957 - val_acc: 0.7540\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5931 - acc: 0.7979 - val_loss: 0.6895 - val_acc: 0.7620\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5870 - acc: 0.7980 - val_loss: 0.6870 - val_acc: 0.7620\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5813 - acc: 0.8011 - val_loss: 0.6853 - val_acc: 0.7610\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5751 - acc: 0.8021 - val_loss: 0.6802 - val_acc: 0.7580\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5699 - acc: 0.8055 - val_loss: 0.6773 - val_acc: 0.7610\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5645 - acc: 0.8073 - val_loss: 0.6756 - val_acc: 0.7620\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5593 - acc: 0.8087 - val_loss: 0.6734 - val_acc: 0.7630\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5538 - acc: 0.8108 - val_loss: 0.6710 - val_acc: 0.7640\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5490 - acc: 0.8115 - val_loss: 0.6712 - val_acc: 0.7560\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5442 - acc: 0.8143 - val_loss: 0.6657 - val_acc: 0.7620\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5386 - acc: 0.8179 - val_loss: 0.6661 - val_acc: 0.7650\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5338 - acc: 0.8176 - val_loss: 0.6614 - val_acc: 0.7690\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5292 - acc: 0.8187 - val_loss: 0.6590 - val_acc: 0.7700\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5248 - acc: 0.8196 - val_loss: 0.6558 - val_acc: 0.7640\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5200 - acc: 0.8212 - val_loss: 0.6541 - val_acc: 0.7670\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5157 - acc: 0.8241 - val_loss: 0.6530 - val_acc: 0.7660\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5112 - acc: 0.8245 - val_loss: 0.6520 - val_acc: 0.7700\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5075 - acc: 0.8245 - val_loss: 0.6492 - val_acc: 0.7680\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5032 - acc: 0.8285 - val_loss: 0.6476 - val_acc: 0.7660\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4984 - acc: 0.8305 - val_loss: 0.6471 - val_acc: 0.7640\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4948 - acc: 0.8316 - val_loss: 0.6436 - val_acc: 0.7700\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4903 - acc: 0.8335 - val_loss: 0.6451 - val_acc: 0.7620\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4866 - acc: 0.8352 - val_loss: 0.6407 - val_acc: 0.7710\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4828 - acc: 0.8344 - val_loss: 0.6441 - val_acc: 0.7670\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4787 - acc: 0.8383 - val_loss: 0.6386 - val_acc: 0.7730\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.4751 - acc: 0.8368 - val_loss: 0.6380 - val_acc: 0.7720\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4714 - acc: 0.8391 - val_loss: 0.6382 - val_acc: 0.7700\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4678 - acc: 0.8412 - val_loss: 0.6381 - val_acc: 0.7720\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4637 - acc: 0.8425 - val_loss: 0.6373 - val_acc: 0.7700\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4601 - acc: 0.8452 - val_loss: 0.6331 - val_acc: 0.7740\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4568 - acc: 0.8445 - val_loss: 0.6326 - val_acc: 0.7730\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4532 - acc: 0.8468 - val_loss: 0.6367 - val_acc: 0.7680\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4504 - acc: 0.8473 - val_loss: 0.6358 - val_acc: 0.7710\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4465 - acc: 0.8489 - val_loss: 0.6325 - val_acc: 0.7770\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4434 - acc: 0.8492 - val_loss: 0.6350 - val_acc: 0.7740\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4401 - acc: 0.8512 - val_loss: 0.6312 - val_acc: 0.7790\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4363 - acc: 0.8503 - val_loss: 0.6309 - val_acc: 0.7730\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4328 - acc: 0.8535 - val_loss: 0.6307 - val_acc: 0.7780\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4301 - acc: 0.8544 - val_loss: 0.6264 - val_acc: 0.7770\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4268 - acc: 0.8551 - val_loss: 0.6280 - val_acc: 0.7800\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4235 - acc: 0.8569 - val_loss: 0.6270 - val_acc: 0.7760\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4207 - acc: 0.8588 - val_loss: 0.6276 - val_acc: 0.7830\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4170 - acc: 0.8593 - val_loss: 0.6303 - val_acc: 0.7780\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4142 - acc: 0.8592 - val_loss: 0.6279 - val_acc: 0.7830\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4111 - acc: 0.8616 - val_loss: 0.6266 - val_acc: 0.7820\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4082 - acc: 0.8639 - val_loss: 0.6276 - val_acc: 0.7840\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4052 - acc: 0.8652 - val_loss: 0.6283 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.4021 - acc: 0.8645 - val_loss: 0.6261 - val_acc: 0.7790\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.3992 - acc: 0.8683 - val_loss: 0.6255 - val_acc: 0.7880\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.3964 - acc: 0.8675 - val_loss: 0.6253 - val_acc: 0.7810\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3933 - acc: 0.8680 - val_loss: 0.6264 - val_acc: 0.7840\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3910 - acc: 0.8700 - val_loss: 0.6306 - val_acc: 0.7730\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.3879 - acc: 0.8713 - val_loss: 0.6235 - val_acc: 0.7880\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.3852 - acc: 0.8728 - val_loss: 0.6244 - val_acc: 0.7910\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3824 - acc: 0.8748 - val_loss: 0.6251 - val_acc: 0.7850\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3798 - acc: 0.8745 - val_loss: 0.6274 - val_acc: 0.7800\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3770 - acc: 0.8764 - val_loss: 0.6260 - val_acc: 0.7730\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3743 - acc: 0.8779 - val_loss: 0.6248 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3716 - acc: 0.8785 - val_loss: 0.6254 - val_acc: 0.7900\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3693 - acc: 0.8803 - val_loss: 0.6234 - val_acc: 0.7880\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3663 - acc: 0.8807 - val_loss: 0.6284 - val_acc: 0.7900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3644 - acc: 0.8821 - val_loss: 0.6266 - val_acc: 0.7850\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3612 - acc: 0.8821 - val_loss: 0.6257 - val_acc: 0.7820\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3589 - acc: 0.8852 - val_loss: 0.6272 - val_acc: 0.7870\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 19us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35551452355384827, 0.8859999999682109]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6667816815376282, 0.7599999996821085]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! you remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9265 - acc: 0.2008 - val_loss: 1.9054 - val_acc: 0.2290\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9007 - acc: 0.2313 - val_loss: 1.8785 - val_acc: 0.2530\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8728 - acc: 0.2513 - val_loss: 1.8491 - val_acc: 0.2710\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8416 - acc: 0.2717 - val_loss: 1.8169 - val_acc: 0.2980\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8063 - acc: 0.2992 - val_loss: 1.7800 - val_acc: 0.3270\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7654 - acc: 0.3249 - val_loss: 1.7377 - val_acc: 0.3680\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7195 - acc: 0.3571 - val_loss: 1.6917 - val_acc: 0.3910\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6690 - acc: 0.3912 - val_loss: 1.6413 - val_acc: 0.4060\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6157 - acc: 0.4164 - val_loss: 1.5883 - val_acc: 0.4270\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5603 - acc: 0.4427 - val_loss: 1.5345 - val_acc: 0.4550\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5035 - acc: 0.4764 - val_loss: 1.4788 - val_acc: 0.4880\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4462 - acc: 0.5135 - val_loss: 1.4228 - val_acc: 0.5300\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3893 - acc: 0.5427 - val_loss: 1.3683 - val_acc: 0.5530\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3342 - acc: 0.5707 - val_loss: 1.3183 - val_acc: 0.5680\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2815 - acc: 0.5908 - val_loss: 1.2681 - val_acc: 0.6110\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2312 - acc: 0.6240 - val_loss: 1.2200 - val_acc: 0.6310\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1833 - acc: 0.6437 - val_loss: 1.1761 - val_acc: 0.6480\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1380 - acc: 0.6609 - val_loss: 1.1348 - val_acc: 0.6570\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0958 - acc: 0.6715 - val_loss: 1.0970 - val_acc: 0.6680\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0563 - acc: 0.6839 - val_loss: 1.0606 - val_acc: 0.6820\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0190 - acc: 0.6933 - val_loss: 1.0289 - val_acc: 0.6940\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9854 - acc: 0.7007 - val_loss: 1.0010 - val_acc: 0.6980\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9543 - acc: 0.7097 - val_loss: 0.9705 - val_acc: 0.7100\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9249 - acc: 0.7173 - val_loss: 0.9478 - val_acc: 0.7140\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8981 - acc: 0.7243 - val_loss: 0.9232 - val_acc: 0.7140\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8735 - acc: 0.7288 - val_loss: 0.9046 - val_acc: 0.7100\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8504 - acc: 0.7349 - val_loss: 0.8838 - val_acc: 0.7220\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8296 - acc: 0.7393 - val_loss: 0.8655 - val_acc: 0.7270\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8099 - acc: 0.7431 - val_loss: 0.8494 - val_acc: 0.7240\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7914 - acc: 0.7485 - val_loss: 0.8360 - val_acc: 0.7290\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7745 - acc: 0.7501 - val_loss: 0.8227 - val_acc: 0.7320\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7591 - acc: 0.7556 - val_loss: 0.8082 - val_acc: 0.7350\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7444 - acc: 0.7593 - val_loss: 0.7989 - val_acc: 0.7350\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7304 - acc: 0.7612 - val_loss: 0.7871 - val_acc: 0.7340\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7174 - acc: 0.7659 - val_loss: 0.7779 - val_acc: 0.7340\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7056 - acc: 0.7699 - val_loss: 0.7680 - val_acc: 0.7360\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6943 - acc: 0.7719 - val_loss: 0.7596 - val_acc: 0.7400\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6831 - acc: 0.7745 - val_loss: 0.7527 - val_acc: 0.7400\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6727 - acc: 0.7765 - val_loss: 0.7437 - val_acc: 0.7470\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6630 - acc: 0.7808 - val_loss: 0.7377 - val_acc: 0.7490\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6534 - acc: 0.7835 - val_loss: 0.7301 - val_acc: 0.7500\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6445 - acc: 0.7853 - val_loss: 0.7247 - val_acc: 0.7400\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6359 - acc: 0.7881 - val_loss: 0.7225 - val_acc: 0.7560\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6281 - acc: 0.7905 - val_loss: 0.7156 - val_acc: 0.7480\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6200 - acc: 0.7925 - val_loss: 0.7117 - val_acc: 0.7610\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6129 - acc: 0.7953 - val_loss: 0.7068 - val_acc: 0.7550\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6052 - acc: 0.7957 - val_loss: 0.6990 - val_acc: 0.7620\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5977 - acc: 0.7981 - val_loss: 0.6978 - val_acc: 0.7620\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5909 - acc: 0.8011 - val_loss: 0.6918 - val_acc: 0.7630\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5840 - acc: 0.8021 - val_loss: 0.6926 - val_acc: 0.7710\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5784 - acc: 0.8019 - val_loss: 0.6862 - val_acc: 0.7660\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5714 - acc: 0.8067 - val_loss: 0.6840 - val_acc: 0.7650\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5654 - acc: 0.8092 - val_loss: 0.6783 - val_acc: 0.7710\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5597 - acc: 0.8093 - val_loss: 0.6766 - val_acc: 0.7680\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5539 - acc: 0.8137 - val_loss: 0.6732 - val_acc: 0.7730\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5484 - acc: 0.8133 - val_loss: 0.6697 - val_acc: 0.7710\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5427 - acc: 0.8152 - val_loss: 0.6702 - val_acc: 0.7770\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5378 - acc: 0.8148 - val_loss: 0.6665 - val_acc: 0.7700\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5325 - acc: 0.8185 - val_loss: 0.6657 - val_acc: 0.7670\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5272 - acc: 0.8209 - val_loss: 0.6600 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5689828497727712, 0.8097333333651224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7319343857765198, 0.7146666668256124]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=kernel_regulizers.l2(lamda_coeff)` parameter to any model layer. The lambda_coeff parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.6098 - acc: 0.1491 - val_loss: 2.5914 - val_acc: 0.1860\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5848 - acc: 0.1917 - val_loss: 2.5684 - val_acc: 0.2220\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.5609 - acc: 0.2268 - val_loss: 2.5457 - val_acc: 0.2460\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.5355 - acc: 0.2508 - val_loss: 2.5202 - val_acc: 0.2750\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5070 - acc: 0.2744 - val_loss: 2.4902 - val_acc: 0.3020\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.4741 - acc: 0.2961 - val_loss: 2.4553 - val_acc: 0.3240\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.4360 - acc: 0.3268 - val_loss: 2.4150 - val_acc: 0.3620\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3918 - acc: 0.3603 - val_loss: 2.3689 - val_acc: 0.3930\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3419 - acc: 0.4016 - val_loss: 2.3186 - val_acc: 0.4330\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2872 - acc: 0.4397 - val_loss: 2.2646 - val_acc: 0.4790\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2286 - acc: 0.4807 - val_loss: 2.2083 - val_acc: 0.5030\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1680 - acc: 0.5124 - val_loss: 2.1494 - val_acc: 0.5240\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1059 - acc: 0.5416 - val_loss: 2.0914 - val_acc: 0.5550\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0447 - acc: 0.5709 - val_loss: 2.0342 - val_acc: 0.5740\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9848 - acc: 0.5988 - val_loss: 1.9780 - val_acc: 0.5950\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9270 - acc: 0.6199 - val_loss: 1.9240 - val_acc: 0.6040\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8720 - acc: 0.6400 - val_loss: 1.8732 - val_acc: 0.6240\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8199 - acc: 0.6511 - val_loss: 1.8261 - val_acc: 0.6350\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7705 - acc: 0.6672 - val_loss: 1.7806 - val_acc: 0.6490\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7249 - acc: 0.6764 - val_loss: 1.7399 - val_acc: 0.6640\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6821 - acc: 0.6873 - val_loss: 1.7000 - val_acc: 0.6620\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6427 - acc: 0.6959 - val_loss: 1.6654 - val_acc: 0.6760\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6062 - acc: 0.7040 - val_loss: 1.6323 - val_acc: 0.6730\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5729 - acc: 0.7132 - val_loss: 1.6008 - val_acc: 0.6890\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5418 - acc: 0.7188 - val_loss: 1.5732 - val_acc: 0.6930\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5129 - acc: 0.7260 - val_loss: 1.5490 - val_acc: 0.6930\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4868 - acc: 0.7279 - val_loss: 1.5233 - val_acc: 0.7010\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4624 - acc: 0.7347 - val_loss: 1.5033 - val_acc: 0.7140\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4401 - acc: 0.7405 - val_loss: 1.4821 - val_acc: 0.7120\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4188 - acc: 0.7436 - val_loss: 1.4632 - val_acc: 0.7170\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3994 - acc: 0.7475 - val_loss: 1.4457 - val_acc: 0.7240\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3814 - acc: 0.7536 - val_loss: 1.4324 - val_acc: 0.7280\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3646 - acc: 0.7548 - val_loss: 1.4172 - val_acc: 0.7350\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3488 - acc: 0.7587 - val_loss: 1.4014 - val_acc: 0.7340\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3334 - acc: 0.7636 - val_loss: 1.3902 - val_acc: 0.7330\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3196 - acc: 0.7645 - val_loss: 1.3766 - val_acc: 0.7370\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3060 - acc: 0.7679 - val_loss: 1.3656 - val_acc: 0.7360\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2934 - acc: 0.7701 - val_loss: 1.3544 - val_acc: 0.7420\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2814 - acc: 0.7737 - val_loss: 1.3457 - val_acc: 0.7410\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2698 - acc: 0.7743 - val_loss: 1.3346 - val_acc: 0.7450\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2589 - acc: 0.7784 - val_loss: 1.3275 - val_acc: 0.7430\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2483 - acc: 0.7771 - val_loss: 1.3194 - val_acc: 0.7470\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2385 - acc: 0.7796 - val_loss: 1.3098 - val_acc: 0.7490\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2286 - acc: 0.7827 - val_loss: 1.3031 - val_acc: 0.7490\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2194 - acc: 0.7863 - val_loss: 1.2945 - val_acc: 0.7500\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2103 - acc: 0.7872 - val_loss: 1.2856 - val_acc: 0.7540\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2020 - acc: 0.7875 - val_loss: 1.2808 - val_acc: 0.7630\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1935 - acc: 0.7908 - val_loss: 1.2754 - val_acc: 0.7630\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1853 - acc: 0.7925 - val_loss: 1.2689 - val_acc: 0.7560\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1776 - acc: 0.7931 - val_loss: 1.2637 - val_acc: 0.7660\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1695 - acc: 0.7968 - val_loss: 1.2577 - val_acc: 0.7610\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1621 - acc: 0.7975 - val_loss: 1.2507 - val_acc: 0.7640\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1548 - acc: 0.7971 - val_loss: 1.2463 - val_acc: 0.7670\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1474 - acc: 0.8005 - val_loss: 1.2406 - val_acc: 0.7670\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1405 - acc: 0.8015 - val_loss: 1.2345 - val_acc: 0.7640\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1335 - acc: 0.8024 - val_loss: 1.2315 - val_acc: 0.7700\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1269 - acc: 0.8061 - val_loss: 1.2244 - val_acc: 0.7670\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1201 - acc: 0.8075 - val_loss: 1.2209 - val_acc: 0.7540\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1138 - acc: 0.8084 - val_loss: 1.2172 - val_acc: 0.7640\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1076 - acc: 0.8092 - val_loss: 1.2129 - val_acc: 0.7640\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1013 - acc: 0.8111 - val_loss: 1.2069 - val_acc: 0.7650\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0954 - acc: 0.8141 - val_loss: 1.2027 - val_acc: 0.7660\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0894 - acc: 0.8141 - val_loss: 1.1984 - val_acc: 0.7660\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0832 - acc: 0.8164 - val_loss: 1.1934 - val_acc: 0.7690\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0774 - acc: 0.8172 - val_loss: 1.1931 - val_acc: 0.7670\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0719 - acc: 0.8175 - val_loss: 1.1883 - val_acc: 0.7650\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0661 - acc: 0.8201 - val_loss: 1.1909 - val_acc: 0.7710\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0611 - acc: 0.8217 - val_loss: 1.1782 - val_acc: 0.7650\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0551 - acc: 0.8207 - val_loss: 1.1759 - val_acc: 0.7750\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0499 - acc: 0.8228 - val_loss: 1.1726 - val_acc: 0.7650\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0446 - acc: 0.8243 - val_loss: 1.1696 - val_acc: 0.7650\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0392 - acc: 0.8251 - val_loss: 1.1675 - val_acc: 0.7660\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0344 - acc: 0.8285 - val_loss: 1.1637 - val_acc: 0.7650\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0289 - acc: 0.8308 - val_loss: 1.1586 - val_acc: 0.7670\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0238 - acc: 0.8292 - val_loss: 1.1569 - val_acc: 0.7630\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0188 - acc: 0.8308 - val_loss: 1.1521 - val_acc: 0.7680\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0140 - acc: 0.8321 - val_loss: 1.1497 - val_acc: 0.7710\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0087 - acc: 0.8340 - val_loss: 1.1451 - val_acc: 0.7750\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0045 - acc: 0.8349 - val_loss: 1.1441 - val_acc: 0.7690\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9999 - acc: 0.8371 - val_loss: 1.1415 - val_acc: 0.7650\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9950 - acc: 0.8361 - val_loss: 1.1375 - val_acc: 0.7690\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9906 - acc: 0.8388 - val_loss: 1.1382 - val_acc: 0.7750\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9860 - acc: 0.8399 - val_loss: 1.1340 - val_acc: 0.7720\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9814 - acc: 0.8401 - val_loss: 1.1302 - val_acc: 0.7710\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9769 - acc: 0.8392 - val_loss: 1.1273 - val_acc: 0.7770\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9722 - acc: 0.8417 - val_loss: 1.1260 - val_acc: 0.7790\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9681 - acc: 0.8423 - val_loss: 1.1220 - val_acc: 0.7720\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9638 - acc: 0.8445 - val_loss: 1.1189 - val_acc: 0.7750\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9593 - acc: 0.8457 - val_loss: 1.1180 - val_acc: 0.7810\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.8456 - val_loss: 1.1158 - val_acc: 0.7770\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9509 - acc: 0.8476 - val_loss: 1.1122 - val_acc: 0.7770\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9467 - acc: 0.8491 - val_loss: 1.1090 - val_acc: 0.7820\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9428 - acc: 0.8504 - val_loss: 1.1045 - val_acc: 0.7860\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9384 - acc: 0.8491 - val_loss: 1.1045 - val_acc: 0.7830\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9343 - acc: 0.8517 - val_loss: 1.1016 - val_acc: 0.7800\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9302 - acc: 0.8520 - val_loss: 1.0998 - val_acc: 0.7800\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9263 - acc: 0.8535 - val_loss: 1.1000 - val_acc: 0.7790\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9225 - acc: 0.8548 - val_loss: 1.0940 - val_acc: 0.7830\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9184 - acc: 0.8548 - val_loss: 1.0951 - val_acc: 0.7750\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9145 - acc: 0.8584 - val_loss: 1.0898 - val_acc: 0.7870\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9108 - acc: 0.8581 - val_loss: 1.0885 - val_acc: 0.7820\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9067 - acc: 0.8572 - val_loss: 1.0849 - val_acc: 0.7860\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9038 - acc: 0.8583 - val_loss: 1.0848 - val_acc: 0.7830\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8993 - acc: 0.8599 - val_loss: 1.0829 - val_acc: 0.7820\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8961 - acc: 0.8613 - val_loss: 1.0797 - val_acc: 0.7830\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8920 - acc: 0.8604 - val_loss: 1.0775 - val_acc: 0.7770\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8885 - acc: 0.8619 - val_loss: 1.0791 - val_acc: 0.7740\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8848 - acc: 0.8624 - val_loss: 1.0750 - val_acc: 0.7820\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8812 - acc: 0.8640 - val_loss: 1.0728 - val_acc: 0.7830\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8777 - acc: 0.8639 - val_loss: 1.0700 - val_acc: 0.7790\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8741 - acc: 0.8636 - val_loss: 1.0695 - val_acc: 0.7750\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8703 - acc: 0.8664 - val_loss: 1.0658 - val_acc: 0.7960\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8670 - acc: 0.8671 - val_loss: 1.0645 - val_acc: 0.7810\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8639 - acc: 0.8676 - val_loss: 1.0645 - val_acc: 0.7770\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8602 - acc: 0.8685 - val_loss: 1.0653 - val_acc: 0.7760\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8570 - acc: 0.8696 - val_loss: 1.0605 - val_acc: 0.7840\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8534 - acc: 0.8720 - val_loss: 1.0579 - val_acc: 0.7780\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8499 - acc: 0.8709 - val_loss: 1.0573 - val_acc: 0.7810\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8470 - acc: 0.8708 - val_loss: 1.0544 - val_acc: 0.7910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8434 - acc: 0.8707 - val_loss: 1.0525 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 15.9956 - acc: 0.1545 - val_loss: 15.5879 - val_acc: 0.1840\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 15.2333 - acc: 0.1917 - val_loss: 14.8415 - val_acc: 0.2200\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 14.4974 - acc: 0.2177 - val_loss: 14.1175 - val_acc: 0.2340\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.7831 - acc: 0.2445 - val_loss: 13.4147 - val_acc: 0.2740\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.0893 - acc: 0.2736 - val_loss: 12.7320 - val_acc: 0.3060\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 12.4152 - acc: 0.3052 - val_loss: 12.0687 - val_acc: 0.3300\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 11.7608 - acc: 0.3321 - val_loss: 11.4256 - val_acc: 0.3470\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 11.1266 - acc: 0.3563 - val_loss: 10.8029 - val_acc: 0.3620\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 10.5128 - acc: 0.3748 - val_loss: 10.2002 - val_acc: 0.3890\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.9194 - acc: 0.3983 - val_loss: 9.6189 - val_acc: 0.4150\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 9.3466 - acc: 0.4144 - val_loss: 9.0578 - val_acc: 0.4480\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 8.7944 - acc: 0.4412 - val_loss: 8.5193 - val_acc: 0.4720\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.2639 - acc: 0.4628 - val_loss: 7.9987 - val_acc: 0.4760\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.7546 - acc: 0.4731 - val_loss: 7.5009 - val_acc: 0.5040\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 7.2671 - acc: 0.4980 - val_loss: 7.0261 - val_acc: 0.5340\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 6.8018 - acc: 0.5132 - val_loss: 6.5729 - val_acc: 0.5430\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.3585 - acc: 0.5291 - val_loss: 6.1429 - val_acc: 0.5580\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9383 - acc: 0.5479 - val_loss: 5.7327 - val_acc: 0.5570\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5407 - acc: 0.5483 - val_loss: 5.3480 - val_acc: 0.5700\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.1660 - acc: 0.5625 - val_loss: 4.9857 - val_acc: 0.5760\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 4.8146 - acc: 0.5697 - val_loss: 4.6454 - val_acc: 0.5770\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 4.4847 - acc: 0.5767 - val_loss: 4.3290 - val_acc: 0.5940\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.1782 - acc: 0.5819 - val_loss: 4.0342 - val_acc: 0.6030\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8937 - acc: 0.5875 - val_loss: 3.7623 - val_acc: 0.5980\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.6324 - acc: 0.5928 - val_loss: 3.5145 - val_acc: 0.5990\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.3931 - acc: 0.5988 - val_loss: 3.2844 - val_acc: 0.6060\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.1758 - acc: 0.6027 - val_loss: 3.0780 - val_acc: 0.6020\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9799 - acc: 0.6008 - val_loss: 2.8926 - val_acc: 0.6120\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.8050 - acc: 0.6088 - val_loss: 2.7291 - val_acc: 0.6030\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.6513 - acc: 0.6063 - val_loss: 2.5865 - val_acc: 0.6160\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.5179 - acc: 0.6121 - val_loss: 2.4618 - val_acc: 0.6140\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.4041 - acc: 0.6152 - val_loss: 2.3569 - val_acc: 0.6230\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3097 - acc: 0.6149 - val_loss: 2.2733 - val_acc: 0.6290\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.2340 - acc: 0.6208 - val_loss: 2.2042 - val_acc: 0.6210\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1751 - acc: 0.6213 - val_loss: 2.1538 - val_acc: 0.6280\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1311 - acc: 0.6228 - val_loss: 2.1153 - val_acc: 0.6280\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0980 - acc: 0.6285 - val_loss: 2.0870 - val_acc: 0.6300\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0710 - acc: 0.6307 - val_loss: 2.0614 - val_acc: 0.6350\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0475 - acc: 0.6339 - val_loss: 2.0389 - val_acc: 0.6360\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0261 - acc: 0.6356 - val_loss: 2.0186 - val_acc: 0.6340\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0064 - acc: 0.6408 - val_loss: 1.9977 - val_acc: 0.6450\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9878 - acc: 0.6480 - val_loss: 1.9790 - val_acc: 0.6480\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9703 - acc: 0.6512 - val_loss: 1.9645 - val_acc: 0.6550\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9541 - acc: 0.6524 - val_loss: 1.9468 - val_acc: 0.6550\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9375 - acc: 0.6609 - val_loss: 1.9303 - val_acc: 0.6650\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9218 - acc: 0.6668 - val_loss: 1.9155 - val_acc: 0.6590\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9067 - acc: 0.6681 - val_loss: 1.9017 - val_acc: 0.6610\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8923 - acc: 0.6723 - val_loss: 1.8868 - val_acc: 0.6660\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8780 - acc: 0.6764 - val_loss: 1.8733 - val_acc: 0.6690\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8648 - acc: 0.6756 - val_loss: 1.8583 - val_acc: 0.6750\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8507 - acc: 0.6805 - val_loss: 1.8440 - val_acc: 0.6730\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8380 - acc: 0.6791 - val_loss: 1.8318 - val_acc: 0.6790\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8250 - acc: 0.6801 - val_loss: 1.8208 - val_acc: 0.6770\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8126 - acc: 0.6828 - val_loss: 1.8117 - val_acc: 0.6780\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8006 - acc: 0.6847 - val_loss: 1.8013 - val_acc: 0.6760\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7894 - acc: 0.6859 - val_loss: 1.7860 - val_acc: 0.6850\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7768 - acc: 0.6863 - val_loss: 1.7772 - val_acc: 0.6850\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7660 - acc: 0.6851 - val_loss: 1.7614 - val_acc: 0.6890\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7545 - acc: 0.6900 - val_loss: 1.7505 - val_acc: 0.6910\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7438 - acc: 0.6908 - val_loss: 1.7446 - val_acc: 0.6900\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7329 - acc: 0.6927 - val_loss: 1.7315 - val_acc: 0.6850\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7224 - acc: 0.6911 - val_loss: 1.7189 - val_acc: 0.6910\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7121 - acc: 0.6925 - val_loss: 1.7085 - val_acc: 0.6900\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7022 - acc: 0.6931 - val_loss: 1.7000 - val_acc: 0.6930\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6923 - acc: 0.6943 - val_loss: 1.6936 - val_acc: 0.6910\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6831 - acc: 0.6944 - val_loss: 1.6834 - val_acc: 0.6900\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6735 - acc: 0.6952 - val_loss: 1.6699 - val_acc: 0.6930\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6638 - acc: 0.6963 - val_loss: 1.6642 - val_acc: 0.6960\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6550 - acc: 0.6957 - val_loss: 1.6544 - val_acc: 0.6920\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6455 - acc: 0.6972 - val_loss: 1.6454 - val_acc: 0.6950\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6367 - acc: 0.6967 - val_loss: 1.6362 - val_acc: 0.6970\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6278 - acc: 0.6977 - val_loss: 1.6301 - val_acc: 0.6960\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6195 - acc: 0.6988 - val_loss: 1.6197 - val_acc: 0.6970\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6107 - acc: 0.7001 - val_loss: 1.6097 - val_acc: 0.6990\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6024 - acc: 0.7004 - val_loss: 1.6067 - val_acc: 0.6970\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5941 - acc: 0.7025 - val_loss: 1.5938 - val_acc: 0.6970\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5857 - acc: 0.7019 - val_loss: 1.5876 - val_acc: 0.6960\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5783 - acc: 0.7024 - val_loss: 1.5815 - val_acc: 0.6990\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5697 - acc: 0.7015 - val_loss: 1.5715 - val_acc: 0.7000\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5620 - acc: 0.7039 - val_loss: 1.5630 - val_acc: 0.7020\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5540 - acc: 0.7047 - val_loss: 1.5572 - val_acc: 0.7010\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5458 - acc: 0.7060 - val_loss: 1.5490 - val_acc: 0.7010\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5383 - acc: 0.7068 - val_loss: 1.5467 - val_acc: 0.6960\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5315 - acc: 0.7076 - val_loss: 1.5345 - val_acc: 0.7030\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5236 - acc: 0.7075 - val_loss: 1.5296 - val_acc: 0.7010\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5166 - acc: 0.7075 - val_loss: 1.5190 - val_acc: 0.7030\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5098 - acc: 0.7083 - val_loss: 1.5141 - val_acc: 0.7010\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5020 - acc: 0.7096 - val_loss: 1.5097 - val_acc: 0.6990\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4953 - acc: 0.7096 - val_loss: 1.4989 - val_acc: 0.7050\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4883 - acc: 0.7092 - val_loss: 1.4931 - val_acc: 0.7050\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4816 - acc: 0.7093 - val_loss: 1.4901 - val_acc: 0.7000\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4749 - acc: 0.7107 - val_loss: 1.4803 - val_acc: 0.7020\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4684 - acc: 0.7108 - val_loss: 1.4733 - val_acc: 0.7060\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4619 - acc: 0.7117 - val_loss: 1.4672 - val_acc: 0.7020\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4549 - acc: 0.7123 - val_loss: 1.4614 - val_acc: 0.7020\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4491 - acc: 0.7116 - val_loss: 1.4529 - val_acc: 0.7040\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4421 - acc: 0.7136 - val_loss: 1.4514 - val_acc: 0.7080\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4365 - acc: 0.7136 - val_loss: 1.4412 - val_acc: 0.7060\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4293 - acc: 0.7143 - val_loss: 1.4359 - val_acc: 0.7060\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4243 - acc: 0.7152 - val_loss: 1.4306 - val_acc: 0.7050\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4179 - acc: 0.7133 - val_loss: 1.4273 - val_acc: 0.7070\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4115 - acc: 0.7152 - val_loss: 1.4192 - val_acc: 0.7080\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4055 - acc: 0.7171 - val_loss: 1.4132 - val_acc: 0.7090\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3994 - acc: 0.7152 - val_loss: 1.4070 - val_acc: 0.7130\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3937 - acc: 0.7171 - val_loss: 1.3993 - val_acc: 0.7100\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3877 - acc: 0.7152 - val_loss: 1.4008 - val_acc: 0.7030\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3828 - acc: 0.7168 - val_loss: 1.3951 - val_acc: 0.7070\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3764 - acc: 0.7173 - val_loss: 1.3842 - val_acc: 0.7130\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3708 - acc: 0.7188 - val_loss: 1.3802 - val_acc: 0.7170\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3655 - acc: 0.7177 - val_loss: 1.3771 - val_acc: 0.7090\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3602 - acc: 0.7187 - val_loss: 1.3733 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3542 - acc: 0.7184 - val_loss: 1.3633 - val_acc: 0.7180\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3495 - acc: 0.7181 - val_loss: 1.3571 - val_acc: 0.7180\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3435 - acc: 0.7184 - val_loss: 1.3545 - val_acc: 0.7150\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3384 - acc: 0.7200 - val_loss: 1.3511 - val_acc: 0.7190\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3333 - acc: 0.7204 - val_loss: 1.3450 - val_acc: 0.7220\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3281 - acc: 0.7191 - val_loss: 1.3395 - val_acc: 0.7130\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3236 - acc: 0.7204 - val_loss: 1.3320 - val_acc: 0.7180\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3180 - acc: 0.7207 - val_loss: 1.3290 - val_acc: 0.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3134 - acc: 0.7197 - val_loss: 1.3237 - val_acc: 0.7220\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0140 - acc: 0.1877 - val_loss: 15.6118 - val_acc: 0.2100\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 15.2533 - acc: 0.2067 - val_loss: 14.8668 - val_acc: 0.2240\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 14.5171 - acc: 0.2219 - val_loss: 14.1433 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.8011 - acc: 0.2360 - val_loss: 13.4388 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 13.1038 - acc: 0.2528 - val_loss: 12.7534 - val_acc: 0.2580\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 12.4253 - acc: 0.2772 - val_loss: 12.0869 - val_acc: 0.2720\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7658 - acc: 0.3027 - val_loss: 11.4387 - val_acc: 0.3140\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.1259 - acc: 0.3431 - val_loss: 10.8103 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 10.5062 - acc: 0.3781 - val_loss: 10.2028 - val_acc: 0.3780\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9068 - acc: 0.4124 - val_loss: 9.6137 - val_acc: 0.3970\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3279 - acc: 0.4441 - val_loss: 9.0463 - val_acc: 0.4310\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7708 - acc: 0.4668 - val_loss: 8.5007 - val_acc: 0.4610\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.2357 - acc: 0.4971 - val_loss: 7.9774 - val_acc: 0.4730\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7237 - acc: 0.5213 - val_loss: 7.4776 - val_acc: 0.4990\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2345 - acc: 0.5405 - val_loss: 7.0011 - val_acc: 0.5270\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7686 - acc: 0.5620 - val_loss: 6.5474 - val_acc: 0.5490\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 6.3256 - acc: 0.5787 - val_loss: 6.1182 - val_acc: 0.5860\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.9053 - acc: 0.5941 - val_loss: 5.7067 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.5074 - acc: 0.6119 - val_loss: 5.3218 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 5.1324 - acc: 0.6183 - val_loss: 4.9565 - val_acc: 0.6200\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.7803 - acc: 0.6311 - val_loss: 4.6160 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4521 - acc: 0.6364 - val_loss: 4.2978 - val_acc: 0.6320\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 4.1454 - acc: 0.6468 - val_loss: 4.0029 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.8615 - acc: 0.6531 - val_loss: 3.7284 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5996 - acc: 0.6593 - val_loss: 3.4779 - val_acc: 0.6590\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.3602 - acc: 0.6624 - val_loss: 3.2500 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.1425 - acc: 0.6635 - val_loss: 3.0417 - val_acc: 0.6780\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9463 - acc: 0.6691 - val_loss: 2.8538 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.7710 - acc: 0.6711 - val_loss: 2.6896 - val_acc: 0.6810\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.6167 - acc: 0.6717 - val_loss: 2.5434 - val_acc: 0.6810\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4825 - acc: 0.6703 - val_loss: 2.4194 - val_acc: 0.6810\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.3687 - acc: 0.6731 - val_loss: 2.3143 - val_acc: 0.6780\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2740 - acc: 0.6723 - val_loss: 2.2270 - val_acc: 0.6820\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1980 - acc: 0.6713 - val_loss: 2.1602 - val_acc: 0.6810\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1386 - acc: 0.6725 - val_loss: 2.1087 - val_acc: 0.6800\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0942 - acc: 0.6736 - val_loss: 2.0704 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0616 - acc: 0.6712 - val_loss: 2.0393 - val_acc: 0.6720\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0343 - acc: 0.6735 - val_loss: 2.0143 - val_acc: 0.6710\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0114 - acc: 0.6720 - val_loss: 1.9902 - val_acc: 0.6790\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9903 - acc: 0.6745 - val_loss: 1.9708 - val_acc: 0.6780\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9712 - acc: 0.6747 - val_loss: 1.9519 - val_acc: 0.6830\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9536 - acc: 0.6751 - val_loss: 1.9332 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9369 - acc: 0.6753 - val_loss: 1.9168 - val_acc: 0.6820\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9213 - acc: 0.6769 - val_loss: 1.9012 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9061 - acc: 0.6791 - val_loss: 1.8871 - val_acc: 0.6790\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8920 - acc: 0.6776 - val_loss: 1.8731 - val_acc: 0.6830\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8783 - acc: 0.6800 - val_loss: 1.8585 - val_acc: 0.6820\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8650 - acc: 0.6796 - val_loss: 1.8430 - val_acc: 0.6850\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8519 - acc: 0.6815 - val_loss: 1.8301 - val_acc: 0.6900\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8395 - acc: 0.6824 - val_loss: 1.8186 - val_acc: 0.6910\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8277 - acc: 0.6844 - val_loss: 1.8058 - val_acc: 0.6900\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8157 - acc: 0.6853 - val_loss: 1.7949 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8043 - acc: 0.6844 - val_loss: 1.7821 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7931 - acc: 0.6857 - val_loss: 1.7722 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7828 - acc: 0.6863 - val_loss: 1.7612 - val_acc: 0.6880\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7722 - acc: 0.6876 - val_loss: 1.7494 - val_acc: 0.6940\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7616 - acc: 0.6877 - val_loss: 1.7391 - val_acc: 0.6920\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7517 - acc: 0.6887 - val_loss: 1.7376 - val_acc: 0.6940\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7419 - acc: 0.6892 - val_loss: 1.7217 - val_acc: 0.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7319 - acc: 0.6887 - val_loss: 1.7170 - val_acc: 0.6940\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7227 - acc: 0.6892 - val_loss: 1.7054 - val_acc: 0.6970\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7136 - acc: 0.6903 - val_loss: 1.6913 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7041 - acc: 0.6924 - val_loss: 1.6814 - val_acc: 0.6940\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6951 - acc: 0.6928 - val_loss: 1.6735 - val_acc: 0.7040\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6864 - acc: 0.6944 - val_loss: 1.6635 - val_acc: 0.7020\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6778 - acc: 0.6957 - val_loss: 1.6551 - val_acc: 0.7050\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6691 - acc: 0.6960 - val_loss: 1.6475 - val_acc: 0.7070\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6604 - acc: 0.6987 - val_loss: 1.6402 - val_acc: 0.7010\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6518 - acc: 0.6996 - val_loss: 1.6361 - val_acc: 0.7010\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6436 - acc: 0.6987 - val_loss: 1.6220 - val_acc: 0.7000\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6354 - acc: 0.7015 - val_loss: 1.6156 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6274 - acc: 0.7015 - val_loss: 1.6048 - val_acc: 0.7010\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6198 - acc: 0.7019 - val_loss: 1.6047 - val_acc: 0.7080\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6122 - acc: 0.7016 - val_loss: 1.5891 - val_acc: 0.7060\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6044 - acc: 0.7020 - val_loss: 1.5813 - val_acc: 0.7070\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5962 - acc: 0.7048 - val_loss: 1.5755 - val_acc: 0.7060\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5886 - acc: 0.7039 - val_loss: 1.5679 - val_acc: 0.7060\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5816 - acc: 0.7048 - val_loss: 1.5609 - val_acc: 0.7080\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5739 - acc: 0.7051 - val_loss: 1.5524 - val_acc: 0.7100\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5670 - acc: 0.7047 - val_loss: 1.5454 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5596 - acc: 0.7049 - val_loss: 1.5392 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5523 - acc: 0.7080 - val_loss: 1.5329 - val_acc: 0.7170\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5455 - acc: 0.7063 - val_loss: 1.5254 - val_acc: 0.7150\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7087 - val_loss: 1.5181 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5314 - acc: 0.7088 - val_loss: 1.5132 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5249 - acc: 0.7084 - val_loss: 1.5042 - val_acc: 0.7140\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5176 - acc: 0.7093 - val_loss: 1.5022 - val_acc: 0.7070\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5115 - acc: 0.7108 - val_loss: 1.4923 - val_acc: 0.7130\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5045 - acc: 0.7117 - val_loss: 1.4951 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4983 - acc: 0.7115 - val_loss: 1.4801 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4912 - acc: 0.7127 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4852 - acc: 0.7104 - val_loss: 1.4667 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7136 - val_loss: 1.4594 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4724 - acc: 0.7137 - val_loss: 1.4551 - val_acc: 0.7140\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4658 - acc: 0.7149 - val_loss: 1.4498 - val_acc: 0.7160\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4602 - acc: 0.7151 - val_loss: 1.4421 - val_acc: 0.7170\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7163 - val_loss: 1.4346 - val_acc: 0.7170\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4470 - acc: 0.7149 - val_loss: 1.4340 - val_acc: 0.7200\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4414 - acc: 0.7152 - val_loss: 1.4272 - val_acc: 0.7180\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4362 - acc: 0.7188 - val_loss: 1.4224 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4301 - acc: 0.7153 - val_loss: 1.4136 - val_acc: 0.7190\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4246 - acc: 0.7169 - val_loss: 1.4046 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4182 - acc: 0.7196 - val_loss: 1.4068 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4132 - acc: 0.7200 - val_loss: 1.3998 - val_acc: 0.7190\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4072 - acc: 0.7176 - val_loss: 1.3891 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4016 - acc: 0.7204 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3966 - acc: 0.7208 - val_loss: 1.3825 - val_acc: 0.7210\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3906 - acc: 0.7225 - val_loss: 1.3804 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3854 - acc: 0.7219 - val_loss: 1.3692 - val_acc: 0.7220\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3802 - acc: 0.7215 - val_loss: 1.3651 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3750 - acc: 0.7201 - val_loss: 1.3593 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3703 - acc: 0.7241 - val_loss: 1.3563 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3650 - acc: 0.7241 - val_loss: 1.3489 - val_acc: 0.7250\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3596 - acc: 0.7249 - val_loss: 1.3460 - val_acc: 0.7250\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3543 - acc: 0.7255 - val_loss: 1.3375 - val_acc: 0.7250\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7248 - val_loss: 1.3340 - val_acc: 0.7270\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3450 - acc: 0.7249 - val_loss: 1.3339 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3404 - acc: 0.7247 - val_loss: 1.3267 - val_acc: 0.7210\n",
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3356 - acc: 0.7267 - val_loss: 1.3237 - val_acc: 0.7160\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3309 - acc: 0.7299 - val_loss: 1.3153 - val_acc: 0.7290\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3262 - acc: 0.7273 - val_loss: 1.3085 - val_acc: 0.7270\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3214 - acc: 0.7275 - val_loss: 1.3051 - val_acc: 0.7210\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3167 - acc: 0.7279 - val_loss: 1.3032 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3118 - acc: 0.7288 - val_loss: 1.2985 - val_acc: 0.7240\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3073 - acc: 0.7301 - val_loss: 1.2923 - val_acc: 0.7240\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3030 - acc: 0.7299 - val_loss: 1.2887 - val_acc: 0.7270\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2984 - acc: 0.7301 - val_loss: 1.2880 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2947 - acc: 0.7303 - val_loss: 1.2849 - val_acc: 0.7180\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2899 - acc: 0.7319 - val_loss: 1.2752 - val_acc: 0.7220\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2858 - acc: 0.7301 - val_loss: 1.2707 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2819 - acc: 0.7319 - val_loss: 1.2682 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2771 - acc: 0.7319 - val_loss: 1.2628 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2734 - acc: 0.7331 - val_loss: 1.2590 - val_acc: 0.7290\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2691 - acc: 0.7335 - val_loss: 1.2582 - val_acc: 0.7250\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2655 - acc: 0.7348 - val_loss: 1.2505 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2614 - acc: 0.7351 - val_loss: 1.2478 - val_acc: 0.7320\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2575 - acc: 0.7325 - val_loss: 1.2434 - val_acc: 0.7270\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2538 - acc: 0.7355 - val_loss: 1.2403 - val_acc: 0.7290\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2497 - acc: 0.7355 - val_loss: 1.2362 - val_acc: 0.7290\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2456 - acc: 0.7341 - val_loss: 1.2344 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2418 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2380 - acc: 0.7364 - val_loss: 1.2267 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2344 - acc: 0.7365 - val_loss: 1.2225 - val_acc: 0.7280\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2306 - acc: 0.7371 - val_loss: 1.2170 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2272 - acc: 0.7349 - val_loss: 1.2162 - val_acc: 0.7320\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2233 - acc: 0.7385 - val_loss: 1.2114 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2199 - acc: 0.7385 - val_loss: 1.2098 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2166 - acc: 0.7379 - val_loss: 1.2034 - val_acc: 0.7340\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2128 - acc: 0.7393 - val_loss: 1.2085 - val_acc: 0.7230\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2096 - acc: 0.7389 - val_loss: 1.1977 - val_acc: 0.7310\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2066 - acc: 0.7391 - val_loss: 1.1958 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2030 - acc: 0.7389 - val_loss: 1.1909 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1992 - acc: 0.7396 - val_loss: 1.1875 - val_acc: 0.7350\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1969 - acc: 0.7380 - val_loss: 1.1904 - val_acc: 0.7290\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1932 - acc: 0.7395 - val_loss: 1.1811 - val_acc: 0.7340\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1896 - acc: 0.7405 - val_loss: 1.1802 - val_acc: 0.7370\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1861 - acc: 0.7408 - val_loss: 1.1754 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1834 - acc: 0.7409 - val_loss: 1.1755 - val_acc: 0.7320\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1803 - acc: 0.7425 - val_loss: 1.1680 - val_acc: 0.7360\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1766 - acc: 0.7424 - val_loss: 1.1751 - val_acc: 0.7280\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1752 - acc: 0.7417 - val_loss: 1.1609 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1716 - acc: 0.7423 - val_loss: 1.1612 - val_acc: 0.7360\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1687 - acc: 0.7409 - val_loss: 1.1594 - val_acc: 0.7370\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1658 - acc: 0.7441 - val_loss: 1.1571 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1629 - acc: 0.7419 - val_loss: 1.1572 - val_acc: 0.7390\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1598 - acc: 0.7429 - val_loss: 1.1513 - val_acc: 0.7430\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1577 - acc: 0.7429 - val_loss: 1.1462 - val_acc: 0.7390\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1546 - acc: 0.7427 - val_loss: 1.1453 - val_acc: 0.7410\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1517 - acc: 0.7431 - val_loss: 1.1406 - val_acc: 0.7410\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1490 - acc: 0.7461 - val_loss: 1.1447 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1470 - acc: 0.7456 - val_loss: 1.1377 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1438 - acc: 0.7455 - val_loss: 1.1356 - val_acc: 0.7440\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1415 - acc: 0.7457 - val_loss: 1.1324 - val_acc: 0.7390\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1394 - acc: 0.7433 - val_loss: 1.1291 - val_acc: 0.7370\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1362 - acc: 0.7455 - val_loss: 1.1279 - val_acc: 0.7440\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7449 - val_loss: 1.1235 - val_acc: 0.7370\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1320 - acc: 0.7447 - val_loss: 1.1211 - val_acc: 0.7380\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1293 - acc: 0.7463 - val_loss: 1.1236 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1279 - acc: 0.7457 - val_loss: 1.1186 - val_acc: 0.7400\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1250 - acc: 0.7467 - val_loss: 1.1250 - val_acc: 0.7390\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1234 - acc: 0.7436 - val_loss: 1.1210 - val_acc: 0.7360\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1214 - acc: 0.7468 - val_loss: 1.1116 - val_acc: 0.7430\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1188 - acc: 0.7488 - val_loss: 1.1138 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1174 - acc: 0.7473 - val_loss: 1.1087 - val_acc: 0.7440\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1145 - acc: 0.7479 - val_loss: 1.1078 - val_acc: 0.7410\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1129 - acc: 0.7463 - val_loss: 1.1084 - val_acc: 0.7450\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1110 - acc: 0.7476 - val_loss: 1.1017 - val_acc: 0.7440\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1081 - acc: 0.7471 - val_loss: 1.1027 - val_acc: 0.7410\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1065 - acc: 0.7489 - val_loss: 1.0996 - val_acc: 0.7380\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1045 - acc: 0.7473 - val_loss: 1.0977 - val_acc: 0.7430\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1028 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7370\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1010 - acc: 0.7475 - val_loss: 1.0954 - val_acc: 0.7370\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0997 - acc: 0.7484 - val_loss: 1.0904 - val_acc: 0.7450\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0972 - acc: 0.7500 - val_loss: 1.0884 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0953 - acc: 0.7503 - val_loss: 1.0877 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0937 - acc: 0.7480 - val_loss: 1.0867 - val_acc: 0.7390\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0922 - acc: 0.7485 - val_loss: 1.0866 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0896 - acc: 0.7497 - val_loss: 1.0832 - val_acc: 0.7440\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0879 - acc: 0.7495 - val_loss: 1.0827 - val_acc: 0.7440\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7511 - val_loss: 1.0830 - val_acc: 0.7430\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.7515 - val_loss: 1.0847 - val_acc: 0.7440\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0835 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7420\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0813 - acc: 0.7508 - val_loss: 1.0765 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0800 - acc: 0.7508 - val_loss: 1.0759 - val_acc: 0.7480\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0782 - acc: 0.7513 - val_loss: 1.0711 - val_acc: 0.7430\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0765 - acc: 0.7496 - val_loss: 1.0708 - val_acc: 0.7420\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0753 - acc: 0.7517 - val_loss: 1.0696 - val_acc: 0.7420\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0740 - acc: 0.7512 - val_loss: 1.0685 - val_acc: 0.7400\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0720 - acc: 0.7521 - val_loss: 1.0647 - val_acc: 0.7430\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0696 - acc: 0.7523 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7537 - val_loss: 1.0667 - val_acc: 0.7420\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0681 - acc: 0.7524 - val_loss: 1.0655 - val_acc: 0.7390\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0656 - acc: 0.7537 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0650 - acc: 0.7543 - val_loss: 1.0602 - val_acc: 0.7420\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0624 - acc: 0.7532 - val_loss: 1.0579 - val_acc: 0.7480\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7535 - val_loss: 1.0581 - val_acc: 0.7390\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0599 - acc: 0.7533 - val_loss: 1.0526 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0584 - acc: 0.7539 - val_loss: 1.0534 - val_acc: 0.7440\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0572 - acc: 0.7547 - val_loss: 1.0521 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0560 - acc: 0.7552 - val_loss: 1.0489 - val_acc: 0.7490\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0544 - acc: 0.7543 - val_loss: 1.0516 - val_acc: 0.7460\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0530 - acc: 0.7552 - val_loss: 1.0571 - val_acc: 0.7400\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0526 - acc: 0.7544 - val_loss: 1.0492 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0509 - acc: 0.7527 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0493 - acc: 0.7540 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0479 - acc: 0.7555 - val_loss: 1.0413 - val_acc: 0.7430\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0465 - acc: 0.7545 - val_loss: 1.0415 - val_acc: 0.7440\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0449 - acc: 0.7561 - val_loss: 1.0418 - val_acc: 0.7440\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0438 - acc: 0.7551 - val_loss: 1.0408 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0421 - acc: 0.7563 - val_loss: 1.0380 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0410 - acc: 0.7564 - val_loss: 1.0371 - val_acc: 0.7440\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0400 - acc: 0.7569 - val_loss: 1.0419 - val_acc: 0.7370\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0390 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7450\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0372 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7400\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0359 - acc: 0.7575 - val_loss: 1.0348 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0357 - acc: 0.7561 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0334 - acc: 0.7564 - val_loss: 1.0296 - val_acc: 0.7510\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0328 - acc: 0.7564 - val_loss: 1.0358 - val_acc: 0.7510\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0323 - acc: 0.7555 - val_loss: 1.0284 - val_acc: 0.7460\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0306 - acc: 0.7579 - val_loss: 1.0279 - val_acc: 0.7490\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0292 - acc: 0.7588 - val_loss: 1.0257 - val_acc: 0.7470\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0285 - acc: 0.7576 - val_loss: 1.0270 - val_acc: 0.7450\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0269 - acc: 0.7560 - val_loss: 1.0248 - val_acc: 0.7420\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0254 - val_acc: 0.7450\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0244 - acc: 0.7569 - val_loss: 1.0243 - val_acc: 0.7450\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0233 - acc: 0.7599 - val_loss: 1.0241 - val_acc: 0.7460\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0225 - acc: 0.7581 - val_loss: 1.0215 - val_acc: 0.7440\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0215 - acc: 0.7583 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0202 - acc: 0.7609 - val_loss: 1.0187 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0196 - acc: 0.7559 - val_loss: 1.0175 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0179 - acc: 0.7597 - val_loss: 1.0187 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0173 - acc: 0.7612 - val_loss: 1.0198 - val_acc: 0.7440\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0165 - acc: 0.7596 - val_loss: 1.0164 - val_acc: 0.7520\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0160 - acc: 0.7591 - val_loss: 1.0137 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0143 - acc: 0.7608 - val_loss: 1.0162 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7603 - val_loss: 1.0109 - val_acc: 0.7510\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0121 - acc: 0.7615 - val_loss: 1.0209 - val_acc: 0.7530\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0118 - acc: 0.7607 - val_loss: 1.0087 - val_acc: 0.7530\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0103 - acc: 0.7604 - val_loss: 1.0091 - val_acc: 0.7490\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0092 - acc: 0.7607 - val_loss: 1.0086 - val_acc: 0.7460\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.7613 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0073 - acc: 0.7589 - val_loss: 1.0057 - val_acc: 0.7450\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0056 - acc: 0.7624 - val_loss: 1.0074 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0054 - acc: 0.7617 - val_loss: 1.0050 - val_acc: 0.7430\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0038 - acc: 0.7629 - val_loss: 1.0060 - val_acc: 0.7520\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0037 - acc: 0.7595 - val_loss: 1.0063 - val_acc: 0.7470\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0023 - acc: 0.7624 - val_loss: 1.0027 - val_acc: 0.7550\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0012 - acc: 0.7624 - val_loss: 1.0099 - val_acc: 0.7470\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0015 - acc: 0.7605 - val_loss: 1.0015 - val_acc: 0.7520\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7640 - val_loss: 0.9987 - val_acc: 0.7490\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9989 - acc: 0.7629 - val_loss: 0.9976 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9978 - acc: 0.7636 - val_loss: 1.0029 - val_acc: 0.7480\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9971 - acc: 0.7621 - val_loss: 0.9973 - val_acc: 0.7540\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9967 - acc: 0.7612 - val_loss: 0.9938 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9952 - acc: 0.7636 - val_loss: 0.9967 - val_acc: 0.7530\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9945 - acc: 0.7635 - val_loss: 1.0000 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7629 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9926 - acc: 0.7643 - val_loss: 0.9954 - val_acc: 0.7530\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9927 - acc: 0.7635 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9908 - acc: 0.7643 - val_loss: 0.9973 - val_acc: 0.7450\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9906 - acc: 0.7633 - val_loss: 0.9918 - val_acc: 0.7570\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9895 - acc: 0.7609 - val_loss: 0.9884 - val_acc: 0.7470\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9884 - acc: 0.7655 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9876 - acc: 0.7648 - val_loss: 0.9932 - val_acc: 0.7490\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9868 - acc: 0.7620 - val_loss: 0.9950 - val_acc: 0.7450\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9861 - acc: 0.7667 - val_loss: 0.9908 - val_acc: 0.7520\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9855 - acc: 0.7635 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9847 - acc: 0.7668 - val_loss: 0.9927 - val_acc: 0.7400\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.7645 - val_loss: 0.9864 - val_acc: 0.7560\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9829 - acc: 0.7653 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9823 - acc: 0.7655 - val_loss: 0.9877 - val_acc: 0.7510\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9815 - acc: 0.7645 - val_loss: 0.9833 - val_acc: 0.7570\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9808 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7520\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9789 - acc: 0.7656 - val_loss: 0.9922 - val_acc: 0.7470\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9784 - acc: 0.7665 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9785 - acc: 0.7648 - val_loss: 0.9781 - val_acc: 0.7530\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9774 - acc: 0.7657 - val_loss: 0.9808 - val_acc: 0.7510\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9768 - acc: 0.7657 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.7667 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9747 - acc: 0.7656 - val_loss: 0.9809 - val_acc: 0.7530\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9749 - acc: 0.7655 - val_loss: 0.9872 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9742 - acc: 0.7675 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9733 - acc: 0.7692 - val_loss: 0.9749 - val_acc: 0.7570\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9723 - acc: 0.7675 - val_loss: 0.9831 - val_acc: 0.7460\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9721 - acc: 0.7671 - val_loss: 0.9876 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9712 - acc: 0.7688 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9705 - acc: 0.7691 - val_loss: 0.9740 - val_acc: 0.7520\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9695 - acc: 0.7700 - val_loss: 0.9739 - val_acc: 0.7550\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7691 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9683 - acc: 0.7680 - val_loss: 0.9741 - val_acc: 0.7440\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9672 - acc: 0.7667 - val_loss: 0.9804 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9665 - acc: 0.7671 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9658 - acc: 0.7677 - val_loss: 0.9884 - val_acc: 0.7550\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.7697 - val_loss: 0.9716 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9652 - acc: 0.7655 - val_loss: 0.9746 - val_acc: 0.7520\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9644 - acc: 0.7688 - val_loss: 0.9681 - val_acc: 0.7560\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9641 - acc: 0.7685 - val_loss: 0.9718 - val_acc: 0.7510\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7688 - val_loss: 0.9729 - val_acc: 0.7540\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9627 - acc: 0.7673 - val_loss: 0.9664 - val_acc: 0.7490\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9621 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7699 - val_loss: 0.9635 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9616 - acc: 0.7688 - val_loss: 0.9703 - val_acc: 0.7440\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9603 - acc: 0.7695 - val_loss: 0.9697 - val_acc: 0.7480\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9605 - acc: 0.7669 - val_loss: 0.9734 - val_acc: 0.7450\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7693 - val_loss: 0.9655 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9586 - acc: 0.7687 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7683 - val_loss: 0.9633 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9576 - acc: 0.7681 - val_loss: 0.9751 - val_acc: 0.7540\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9574 - acc: 0.7683 - val_loss: 0.9652 - val_acc: 0.7600\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9572 - acc: 0.7700 - val_loss: 0.9627 - val_acc: 0.7550\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.7677 - val_loss: 0.9716 - val_acc: 0.7520\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9556 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7630\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9555 - acc: 0.7679 - val_loss: 0.9629 - val_acc: 0.7540\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7692 - val_loss: 0.9640 - val_acc: 0.7460\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9541 - acc: 0.7700 - val_loss: 0.9772 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9538 - acc: 0.7713 - val_loss: 0.9612 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9530 - acc: 0.7688 - val_loss: 0.9576 - val_acc: 0.7620\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9510 - acc: 0.7699 - val_loss: 0.9588 - val_acc: 0.7510\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9512 - acc: 0.7692 - val_loss: 0.9565 - val_acc: 0.7580\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9517 - acc: 0.7689 - val_loss: 0.9629 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9504 - acc: 0.7700 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7680 - val_loss: 0.9571 - val_acc: 0.7550\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9496 - acc: 0.7681 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7712 - val_loss: 0.9659 - val_acc: 0.7510\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9487 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7460\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9483 - acc: 0.7709 - val_loss: 0.9521 - val_acc: 0.7580\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9468 - acc: 0.7705 - val_loss: 0.9535 - val_acc: 0.7620\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9469 - acc: 0.7689 - val_loss: 0.9570 - val_acc: 0.7630\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9463 - acc: 0.7717 - val_loss: 0.9548 - val_acc: 0.7530\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9472 - acc: 0.7719 - val_loss: 0.9545 - val_acc: 0.7610\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9461 - acc: 0.7701 - val_loss: 0.9621 - val_acc: 0.7510\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9448 - acc: 0.7716 - val_loss: 0.9535 - val_acc: 0.7650\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9450 - acc: 0.7705 - val_loss: 0.9510 - val_acc: 0.7590\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7703 - val_loss: 0.9564 - val_acc: 0.7510\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9434 - acc: 0.7708 - val_loss: 0.9491 - val_acc: 0.7660\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7720 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9417 - acc: 0.7707 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9429 - acc: 0.7719 - val_loss: 0.9516 - val_acc: 0.7600\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9419 - acc: 0.7729 - val_loss: 0.9532 - val_acc: 0.7620\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9450 - acc: 0.770 - 0s 44us/step - loss: 0.9427 - acc: 0.7716 - val_loss: 0.9465 - val_acc: 0.7610\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9407 - acc: 0.7715 - val_loss: 0.9475 - val_acc: 0.7570\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9408 - acc: 0.7723 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7712 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9392 - acc: 0.7729 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.7712 - val_loss: 0.9449 - val_acc: 0.7540\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9387 - acc: 0.7731 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9381 - acc: 0.7711 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9390 - acc: 0.7708 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9373 - acc: 0.7705 - val_loss: 0.9459 - val_acc: 0.7630\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9374 - acc: 0.7713 - val_loss: 0.9447 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9356 - acc: 0.7717 - val_loss: 0.9426 - val_acc: 0.7650\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7747 - val_loss: 0.9450 - val_acc: 0.7640\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9352 - acc: 0.7708 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7724 - val_loss: 0.9514 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7739 - val_loss: 0.9410 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9336 - acc: 0.7701 - val_loss: 0.9415 - val_acc: 0.7610\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9341 - acc: 0.7712 - val_loss: 0.9454 - val_acc: 0.7580\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9335 - acc: 0.7724 - val_loss: 0.9444 - val_acc: 0.7510\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9326 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9327 - acc: 0.7732 - val_loss: 0.9391 - val_acc: 0.7640\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9313 - acc: 0.7736 - val_loss: 0.9389 - val_acc: 0.7680\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9310 - acc: 0.7736 - val_loss: 0.9373 - val_acc: 0.7640\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9310 - acc: 0.7708 - val_loss: 0.9374 - val_acc: 0.7620\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.7748 - val_loss: 0.9408 - val_acc: 0.7600\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9297 - acc: 0.7739 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9301 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7729 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9287 - acc: 0.7724 - val_loss: 0.9380 - val_acc: 0.7680\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9398 - val_acc: 0.7630\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9281 - acc: 0.7731 - val_loss: 0.9389 - val_acc: 0.7570\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9277 - acc: 0.7737 - val_loss: 0.9360 - val_acc: 0.7630\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7733 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9268 - acc: 0.7725 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9270 - acc: 0.7736 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9268 - acc: 0.7729 - val_loss: 0.9420 - val_acc: 0.7660\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7745 - val_loss: 0.9486 - val_acc: 0.7580\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9260 - acc: 0.7741 - val_loss: 0.9360 - val_acc: 0.7610\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7737 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9237 - acc: 0.7745 - val_loss: 0.9386 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9235 - acc: 0.7752 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9235 - acc: 0.7735 - val_loss: 0.9440 - val_acc: 0.7600\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7725 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9232 - acc: 0.7736 - val_loss: 0.9312 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9228 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7630\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9222 - acc: 0.7721 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9211 - acc: 0.7735 - val_loss: 0.9347 - val_acc: 0.7560\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9210 - acc: 0.7735 - val_loss: 0.9288 - val_acc: 0.7670\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9204 - acc: 0.7720 - val_loss: 0.9274 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7763 - val_loss: 0.9447 - val_acc: 0.7620\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7756 - val_loss: 0.9429 - val_acc: 0.7510\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7733 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9207 - acc: 0.7737 - val_loss: 0.9272 - val_acc: 0.7650\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9191 - acc: 0.7760 - val_loss: 0.9390 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9193 - acc: 0.7736 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9188 - acc: 0.7751 - val_loss: 0.9294 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9179 - acc: 0.7748 - val_loss: 0.9351 - val_acc: 0.7590\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9184 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7670\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9167 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7690\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9168 - acc: 0.7740 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9165 - acc: 0.7756 - val_loss: 0.9301 - val_acc: 0.7620\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7763 - val_loss: 0.9313 - val_acc: 0.7630\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9161 - acc: 0.7743 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7759 - val_loss: 0.9281 - val_acc: 0.7530\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9152 - acc: 0.7751 - val_loss: 0.9285 - val_acc: 0.7660\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7756 - val_loss: 0.9289 - val_acc: 0.7560\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9145 - acc: 0.7756 - val_loss: 0.9243 - val_acc: 0.7690\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9138 - acc: 0.7768 - val_loss: 0.9295 - val_acc: 0.7560\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9137 - acc: 0.7743 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9131 - acc: 0.7760 - val_loss: 0.9238 - val_acc: 0.7650\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9133 - acc: 0.7751 - val_loss: 0.9230 - val_acc: 0.7660\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9129 - acc: 0.7724 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9137 - acc: 0.7729 - val_loss: 0.9236 - val_acc: 0.7670\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7757 - val_loss: 0.9300 - val_acc: 0.7560\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9125 - acc: 0.7745 - val_loss: 0.9280 - val_acc: 0.7640\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7727 - val_loss: 0.9250 - val_acc: 0.7630\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9114 - acc: 0.7753 - val_loss: 0.9233 - val_acc: 0.7580\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9115 - acc: 0.7728 - val_loss: 0.9228 - val_acc: 0.7650\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9119 - acc: 0.7747 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9101 - acc: 0.7753 - val_loss: 0.9370 - val_acc: 0.7540\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9103 - acc: 0.7777 - val_loss: 0.9350 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9286 - val_acc: 0.7580\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.7740 - val_loss: 0.9217 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9085 - acc: 0.7751 - val_loss: 0.9227 - val_acc: 0.7610\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9097 - acc: 0.7771 - val_loss: 0.9202 - val_acc: 0.7650\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9076 - acc: 0.7768 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9082 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7640\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9085 - acc: 0.7753 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9064 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7660\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9064 - acc: 0.7748 - val_loss: 0.9190 - val_acc: 0.7610\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9062 - acc: 0.7765 - val_loss: 0.9218 - val_acc: 0.7620\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9070 - acc: 0.7749 - val_loss: 0.9217 - val_acc: 0.7680\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7764 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7761 - val_loss: 0.9189 - val_acc: 0.7640\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9059 - acc: 0.7767 - val_loss: 0.9195 - val_acc: 0.7640\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9049 - acc: 0.7780 - val_loss: 0.9222 - val_acc: 0.7580\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9043 - acc: 0.7783 - val_loss: 0.9232 - val_acc: 0.7610\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7772 - val_loss: 0.9200 - val_acc: 0.7640\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9046 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9037 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7600\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9035 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7773 - val_loss: 0.9202 - val_acc: 0.7660\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7767 - val_loss: 0.9192 - val_acc: 0.7660\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9171 - val_acc: 0.7670\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9024 - acc: 0.7784 - val_loss: 0.9144 - val_acc: 0.7670\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9019 - acc: 0.7775 - val_loss: 0.9148 - val_acc: 0.7670\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9014 - acc: 0.7773 - val_loss: 0.9198 - val_acc: 0.7520\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9014 - acc: 0.7776 - val_loss: 0.9144 - val_acc: 0.7610\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7767 - val_loss: 0.9141 - val_acc: 0.7670\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7781 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7745 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8994 - acc: 0.7773 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7777 - val_loss: 0.9187 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9140 - val_acc: 0.7620\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8999 - acc: 0.7775 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7792 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8992 - acc: 0.7785 - val_loss: 0.9191 - val_acc: 0.7580\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8987 - acc: 0.7788 - val_loss: 0.9105 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8982 - acc: 0.7780 - val_loss: 0.9209 - val_acc: 0.7580\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7792 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8975 - acc: 0.7781 - val_loss: 0.9128 - val_acc: 0.7690\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8969 - acc: 0.7769 - val_loss: 0.9133 - val_acc: 0.7560\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9196 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8989 - acc: 0.7796 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8973 - acc: 0.7791 - val_loss: 0.9126 - val_acc: 0.7600\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7781 - val_loss: 0.9115 - val_acc: 0.7590\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8961 - acc: 0.7791 - val_loss: 0.9100 - val_acc: 0.7660\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8964 - acc: 0.7771 - val_loss: 0.9090 - val_acc: 0.7660\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8956 - acc: 0.7788 - val_loss: 0.9136 - val_acc: 0.7560\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8959 - acc: 0.7793 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8955 - acc: 0.7792 - val_loss: 0.9107 - val_acc: 0.7670\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8950 - acc: 0.7809 - val_loss: 0.9091 - val_acc: 0.7700\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8940 - acc: 0.7764 - val_loss: 0.9135 - val_acc: 0.7680\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8941 - acc: 0.7785 - val_loss: 0.9099 - val_acc: 0.7600\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8928 - acc: 0.7781 - val_loss: 0.9082 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7785 - val_loss: 0.9113 - val_acc: 0.7620\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8926 - acc: 0.7807 - val_loss: 0.9074 - val_acc: 0.7710\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8939 - acc: 0.7772 - val_loss: 0.9051 - val_acc: 0.7680\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8925 - acc: 0.7781 - val_loss: 0.9156 - val_acc: 0.7650\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7793 - val_loss: 0.9136 - val_acc: 0.7660\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8932 - acc: 0.7797 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8929 - acc: 0.7780 - val_loss: 0.9101 - val_acc: 0.7660\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8921 - acc: 0.7788 - val_loss: 0.9118 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9090 - val_acc: 0.7700\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7797 - val_loss: 0.9057 - val_acc: 0.7640\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8922 - acc: 0.7783 - val_loss: 0.9152 - val_acc: 0.7520\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8911 - acc: 0.7776 - val_loss: 0.9081 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7783 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8913 - acc: 0.7773 - val_loss: 0.9061 - val_acc: 0.7660\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7680\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8894 - acc: 0.7792 - val_loss: 0.9070 - val_acc: 0.7620\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8905 - acc: 0.7783 - val_loss: 0.9043 - val_acc: 0.7690\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.7793 - val_loss: 0.9092 - val_acc: 0.7670\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.7788 - val_loss: 0.9194 - val_acc: 0.7610\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8890 - acc: 0.7799 - val_loss: 0.9028 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8892 - acc: 0.7804 - val_loss: 0.9060 - val_acc: 0.7610\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7793 - val_loss: 0.9112 - val_acc: 0.7640\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8895 - acc: 0.7792 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8881 - acc: 0.7779 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8875 - acc: 0.7791 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8873 - acc: 0.7819 - val_loss: 0.8998 - val_acc: 0.7700\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8862 - acc: 0.7803 - val_loss: 0.9126 - val_acc: 0.7620\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8866 - acc: 0.7807 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8878 - acc: 0.7803 - val_loss: 0.9050 - val_acc: 0.7620\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8868 - acc: 0.7817 - val_loss: 0.9017 - val_acc: 0.7600\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8861 - acc: 0.7813 - val_loss: 0.9029 - val_acc: 0.7710\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8866 - acc: 0.7809 - val_loss: 0.9049 - val_acc: 0.7630\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7808 - val_loss: 0.9038 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8863 - acc: 0.7809 - val_loss: 0.9004 - val_acc: 0.7650\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9061 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7819 - val_loss: 0.9010 - val_acc: 0.7680\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7815 - val_loss: 0.9010 - val_acc: 0.7630\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8846 - acc: 0.7801 - val_loss: 0.9220 - val_acc: 0.7600\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8851 - acc: 0.7827 - val_loss: 0.9006 - val_acc: 0.7710\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7805 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8848 - acc: 0.7805 - val_loss: 0.9001 - val_acc: 0.7680\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8849 - acc: 0.7813 - val_loss: 0.8981 - val_acc: 0.7710\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7795 - val_loss: 0.9019 - val_acc: 0.7670\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8833 - acc: 0.7808 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8842 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7600\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8830 - acc: 0.7813 - val_loss: 0.9020 - val_acc: 0.7700\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8840 - acc: 0.7820 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7812 - val_loss: 0.9092 - val_acc: 0.7610\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8823 - acc: 0.7809 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8825 - acc: 0.7797 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7816 - val_loss: 0.8995 - val_acc: 0.7680\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8814 - acc: 0.7823 - val_loss: 0.9037 - val_acc: 0.7570\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.8956 - val_acc: 0.7760\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8825 - acc: 0.7803 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.8978 - val_acc: 0.7700\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8805 - acc: 0.7829 - val_loss: 0.9006 - val_acc: 0.7690\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8809 - acc: 0.7815 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8807 - acc: 0.7827 - val_loss: 0.8971 - val_acc: 0.7670\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8810 - acc: 0.7813 - val_loss: 0.8964 - val_acc: 0.7720\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9139 - val_acc: 0.7570\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8803 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.7825 - val_loss: 0.9025 - val_acc: 0.7760\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8799 - acc: 0.7827 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8796 - acc: 0.7824 - val_loss: 0.8983 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8794 - acc: 0.7823 - val_loss: 0.8961 - val_acc: 0.7670\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8789 - acc: 0.781 - 0s 36us/step - loss: 0.8798 - acc: 0.7816 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8789 - acc: 0.7817 - val_loss: 0.9079 - val_acc: 0.7670\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8780 - acc: 0.7828 - val_loss: 0.8931 - val_acc: 0.7660\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8794 - acc: 0.7817 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8796 - acc: 0.7829 - val_loss: 0.9164 - val_acc: 0.7620\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8798 - acc: 0.7803 - val_loss: 0.8987 - val_acc: 0.7660\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8778 - acc: 0.7827 - val_loss: 0.9133 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8782 - acc: 0.7827 - val_loss: 0.8960 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9114 - val_acc: 0.7610\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8767 - acc: 0.7835 - val_loss: 0.9009 - val_acc: 0.7680\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8770 - acc: 0.7852 - val_loss: 0.8958 - val_acc: 0.7730\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8760 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7720\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8765 - acc: 0.7804 - val_loss: 0.8937 - val_acc: 0.7740\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8771 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7680\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8775 - acc: 0.7815 - val_loss: 0.8947 - val_acc: 0.7720\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7839 - val_loss: 0.8932 - val_acc: 0.7710\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8765 - acc: 0.7823 - val_loss: 0.8949 - val_acc: 0.7690\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8767 - acc: 0.7823 - val_loss: 0.8966 - val_acc: 0.7650\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8755 - acc: 0.7831 - val_loss: 0.8939 - val_acc: 0.7660\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8768 - acc: 0.7817 - val_loss: 0.8959 - val_acc: 0.7700\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.7820 - val_loss: 0.8954 - val_acc: 0.7690\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8759 - acc: 0.7800 - val_loss: 0.9110 - val_acc: 0.7630\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8749 - acc: 0.7825 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8749 - acc: 0.7833 - val_loss: 0.8952 - val_acc: 0.7670\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8751 - acc: 0.7833 - val_loss: 0.8930 - val_acc: 0.7680\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8744 - acc: 0.7843 - val_loss: 0.8956 - val_acc: 0.7630\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8740 - acc: 0.7827 - val_loss: 0.8945 - val_acc: 0.7690\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8744 - acc: 0.7829 - val_loss: 0.8928 - val_acc: 0.7660\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8747 - acc: 0.7817 - val_loss: 0.8969 - val_acc: 0.7700\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7829 - val_loss: 0.8980 - val_acc: 0.7670\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8726 - acc: 0.7845 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8742 - acc: 0.7841 - val_loss: 0.8928 - val_acc: 0.7720\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8733 - acc: 0.7853 - val_loss: 0.8896 - val_acc: 0.7700\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8725 - acc: 0.7813 - val_loss: 0.8941 - val_acc: 0.7650\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7840 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.7815 - val_loss: 0.8955 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7836 - val_loss: 0.8885 - val_acc: 0.7710\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8718 - acc: 0.7837 - val_loss: 0.8904 - val_acc: 0.7700\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8715 - acc: 0.7837 - val_loss: 0.8936 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8735 - acc: 0.7813 - val_loss: 0.8924 - val_acc: 0.7670\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7855 - val_loss: 0.8929 - val_acc: 0.7710\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7835 - val_loss: 0.8895 - val_acc: 0.7740\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7851 - val_loss: 0.9019 - val_acc: 0.7520\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7836 - val_loss: 0.8904 - val_acc: 0.7680\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7833 - val_loss: 0.8919 - val_acc: 0.7690\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8708 - acc: 0.7847 - val_loss: 0.8945 - val_acc: 0.7730\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8701 - acc: 0.7823 - val_loss: 0.8916 - val_acc: 0.7700\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7828 - val_loss: 0.8978 - val_acc: 0.7550\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.9058 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8698 - acc: 0.7824 - val_loss: 0.8964 - val_acc: 0.7680\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8692 - acc: 0.7851 - val_loss: 0.8976 - val_acc: 0.7690\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8707 - acc: 0.7844 - val_loss: 0.8921 - val_acc: 0.7620\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8695 - acc: 0.7840 - val_loss: 0.8903 - val_acc: 0.7720\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7839 - val_loss: 0.8968 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8697 - acc: 0.7833 - val_loss: 0.8901 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8685 - acc: 0.7851 - val_loss: 0.8876 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8696 - acc: 0.7837 - val_loss: 0.9169 - val_acc: 0.7610\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.7831 - val_loss: 0.8896 - val_acc: 0.7770\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8682 - acc: 0.7828 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7803 - val_loss: 0.8892 - val_acc: 0.7680\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8687 - acc: 0.7824 - val_loss: 0.8875 - val_acc: 0.7770\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8687 - acc: 0.7839 - val_loss: 0.8971 - val_acc: 0.7640\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7848 - val_loss: 0.8876 - val_acc: 0.7740\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7847 - val_loss: 0.9123 - val_acc: 0.7540\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7851 - val_loss: 0.8878 - val_acc: 0.7690\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8683 - acc: 0.7845 - val_loss: 0.8944 - val_acc: 0.7520\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8670 - acc: 0.7845 - val_loss: 0.8923 - val_acc: 0.7690\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8683 - acc: 0.7837 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.8988 - val_acc: 0.7700\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8675 - acc: 0.7851 - val_loss: 0.8952 - val_acc: 0.7700\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7843 - val_loss: 0.8854 - val_acc: 0.7690\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8669 - acc: 0.7835 - val_loss: 0.8869 - val_acc: 0.7720\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8668 - acc: 0.7824 - val_loss: 0.9022 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8677 - acc: 0.7831 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8654 - acc: 0.7859 - val_loss: 0.8909 - val_acc: 0.7730\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7841 - val_loss: 0.8867 - val_acc: 0.7720\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8649 - acc: 0.7823 - val_loss: 0.8942 - val_acc: 0.7660\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7848 - val_loss: 0.8929 - val_acc: 0.7740\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8648 - acc: 0.7843 - val_loss: 0.8940 - val_acc: 0.7730\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8660 - acc: 0.7836 - val_loss: 0.8881 - val_acc: 0.7660\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7864 - val_loss: 0.8970 - val_acc: 0.7670\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8653 - acc: 0.7841 - val_loss: 0.8851 - val_acc: 0.7760\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7836 - val_loss: 0.8896 - val_acc: 0.7760\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7841 - val_loss: 0.9073 - val_acc: 0.7550\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8668 - acc: 0.7856 - val_loss: 0.8923 - val_acc: 0.7630\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8658 - acc: 0.7857 - val_loss: 0.8906 - val_acc: 0.7760\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8642 - acc: 0.7855 - val_loss: 0.8862 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7660\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8647 - acc: 0.7861 - val_loss: 0.9076 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8655 - acc: 0.7853 - val_loss: 0.8893 - val_acc: 0.7700\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8645 - acc: 0.7832 - val_loss: 0.8870 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7843 - val_loss: 0.8907 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7856 - val_loss: 0.8835 - val_acc: 0.7760\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8632 - acc: 0.7845 - val_loss: 0.8857 - val_acc: 0.7690\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8629 - acc: 0.7832 - val_loss: 0.8880 - val_acc: 0.7660\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8625 - acc: 0.7867 - val_loss: 0.8946 - val_acc: 0.7670\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8632 - acc: 0.7867 - val_loss: 0.9199 - val_acc: 0.7470\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7844 - val_loss: 0.8890 - val_acc: 0.7710\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8626 - acc: 0.7867 - val_loss: 0.8937 - val_acc: 0.7620\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8642 - acc: 0.7847 - val_loss: 0.8906 - val_acc: 0.7690\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8622 - acc: 0.7860 - val_loss: 0.8846 - val_acc: 0.7710\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.8870 - val_acc: 0.7670\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8630 - acc: 0.7855 - val_loss: 0.8827 - val_acc: 0.7710\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8619 - acc: 0.7847 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8638 - acc: 0.7860 - val_loss: 0.8872 - val_acc: 0.7560\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8615 - acc: 0.7848 - val_loss: 0.8840 - val_acc: 0.7770\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8606 - acc: 0.7865 - val_loss: 0.8842 - val_acc: 0.7670\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8600 - acc: 0.7875 - val_loss: 0.8842 - val_acc: 0.7740\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8627 - acc: 0.7840 - val_loss: 0.8854 - val_acc: 0.7720\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8617 - acc: 0.7847 - val_loss: 0.8840 - val_acc: 0.7750\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7855 - val_loss: 0.8852 - val_acc: 0.7750\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8610 - acc: 0.7844 - val_loss: 0.8845 - val_acc: 0.7740\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8604 - acc: 0.7869 - val_loss: 0.8938 - val_acc: 0.7700\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8603 - acc: 0.7865 - val_loss: 0.8844 - val_acc: 0.7720\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8603 - acc: 0.7857 - val_loss: 0.8933 - val_acc: 0.7670\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8608 - acc: 0.7860 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.8915 - val_acc: 0.7640\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8605 - acc: 0.7867 - val_loss: 0.8816 - val_acc: 0.7760\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7833 - val_loss: 0.8809 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8585 - acc: 0.7853 - val_loss: 0.8882 - val_acc: 0.7700\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7852 - val_loss: 0.8810 - val_acc: 0.7760\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8584 - acc: 0.7859 - val_loss: 0.8847 - val_acc: 0.7660\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8604 - acc: 0.7852 - val_loss: 0.8929 - val_acc: 0.7670\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8613 - acc: 0.7852 - val_loss: 0.8826 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.7857 - val_loss: 0.8836 - val_acc: 0.7750\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8594 - acc: 0.7865 - val_loss: 0.8780 - val_acc: 0.7750\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8584 - acc: 0.7837 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8569 - acc: 0.7856 - val_loss: 0.9019 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8833 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7867 - val_loss: 0.8867 - val_acc: 0.7670\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7884 - val_loss: 0.8854 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8580 - acc: 0.7863 - val_loss: 0.8930 - val_acc: 0.7600\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.7864 - val_loss: 0.8877 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7864 - val_loss: 0.8848 - val_acc: 0.7670\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8573 - acc: 0.7863 - val_loss: 0.8811 - val_acc: 0.7800\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8580 - acc: 0.7875 - val_loss: 0.8932 - val_acc: 0.7660\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8581 - acc: 0.7884 - val_loss: 0.8864 - val_acc: 0.7630\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8568 - acc: 0.7860 - val_loss: 0.8818 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8582 - acc: 0.7865 - val_loss: 0.8820 - val_acc: 0.7780\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.7875 - val_loss: 0.8899 - val_acc: 0.7640\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7865 - val_loss: 0.8828 - val_acc: 0.7630\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8559 - acc: 0.7872 - val_loss: 0.8801 - val_acc: 0.7730\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8576 - acc: 0.7848 - val_loss: 0.9000 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8566 - acc: 0.7859 - val_loss: 0.8860 - val_acc: 0.7610\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.7876 - val_loss: 0.8777 - val_acc: 0.7790\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7885 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8577 - acc: 0.7881 - val_loss: 0.8791 - val_acc: 0.7750\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8567 - acc: 0.7861 - val_loss: 0.8865 - val_acc: 0.7650\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8570 - acc: 0.7872 - val_loss: 0.8798 - val_acc: 0.7790\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8555 - acc: 0.7875 - val_loss: 0.8794 - val_acc: 0.7690\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8556 - acc: 0.7888 - val_loss: 0.8802 - val_acc: 0.7760\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8554 - acc: 0.7879 - val_loss: 0.8878 - val_acc: 0.7720\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7892 - val_loss: 0.8827 - val_acc: 0.7630\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8553 - acc: 0.7869 - val_loss: 0.8850 - val_acc: 0.7740\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8555 - acc: 0.7856 - val_loss: 0.8881 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8531 - acc: 0.7888 - val_loss: 0.8868 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7872 - val_loss: 0.8804 - val_acc: 0.7740\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8552 - acc: 0.7867 - val_loss: 0.8799 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8550 - acc: 0.7875 - val_loss: 0.8773 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.7867 - val_loss: 0.8778 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8556 - acc: 0.7880 - val_loss: 0.8768 - val_acc: 0.7740\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8545 - acc: 0.7876 - val_loss: 0.8845 - val_acc: 0.7570\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8534 - acc: 0.7879 - val_loss: 0.8850 - val_acc: 0.7700\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8546 - acc: 0.7881 - val_loss: 0.8927 - val_acc: 0.7640\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8547 - acc: 0.7888 - val_loss: 0.8765 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8542 - acc: 0.7877 - val_loss: 0.8846 - val_acc: 0.7620\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8529 - acc: 0.7884 - val_loss: 0.8797 - val_acc: 0.7690\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8535 - acc: 0.7871 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.9048 - val_acc: 0.7520\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8541 - acc: 0.7857 - val_loss: 0.8794 - val_acc: 0.7720\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8543 - acc: 0.7877 - val_loss: 0.8813 - val_acc: 0.7730\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.7877 - val_loss: 0.8790 - val_acc: 0.7610\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7790\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7885 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.7875 - val_loss: 0.8800 - val_acc: 0.7770\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.7863 - val_loss: 0.8798 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7883 - val_loss: 0.8860 - val_acc: 0.7620\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8534 - acc: 0.7867 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8517 - acc: 0.7897 - val_loss: 0.8847 - val_acc: 0.7670\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8513 - acc: 0.7877 - val_loss: 0.8908 - val_acc: 0.7630\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.8952 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8515 - acc: 0.7877 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8524 - acc: 0.7869 - val_loss: 0.8971 - val_acc: 0.7570\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8524 - acc: 0.7879 - val_loss: 0.8761 - val_acc: 0.7690\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8807 - val_acc: 0.7690\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8515 - acc: 0.7876 - val_loss: 0.8847 - val_acc: 0.7680\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.8848 - val_acc: 0.7630\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8516 - acc: 0.7891 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8515 - acc: 0.7881 - val_loss: 0.8771 - val_acc: 0.7740\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8502 - acc: 0.7880 - val_loss: 0.8824 - val_acc: 0.7660\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7897 - val_loss: 0.8898 - val_acc: 0.7520\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8512 - acc: 0.7885 - val_loss: 0.8823 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8509 - acc: 0.7871 - val_loss: 0.8852 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8866 - val_acc: 0.7630\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8504 - acc: 0.7880 - val_loss: 0.9056 - val_acc: 0.7660\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8526 - acc: 0.7872 - val_loss: 0.8780 - val_acc: 0.7710\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8492 - acc: 0.7908 - val_loss: 0.9256 - val_acc: 0.7600\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8505 - acc: 0.7871 - val_loss: 0.8952 - val_acc: 0.7630\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8510 - acc: 0.7888 - val_loss: 0.8824 - val_acc: 0.7680\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.7888 - val_loss: 0.8754 - val_acc: 0.7660\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8494 - acc: 0.7873 - val_loss: 0.8806 - val_acc: 0.7650\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8499 - acc: 0.7888 - val_loss: 0.8784 - val_acc: 0.7610\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.7896 - val_loss: 0.8749 - val_acc: 0.7730\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7883 - val_loss: 0.8865 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8503 - acc: 0.7861 - val_loss: 0.8771 - val_acc: 0.7730\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7860 - val_loss: 0.8727 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8473 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7810\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8500 - acc: 0.7881 - val_loss: 0.8779 - val_acc: 0.7670\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8501 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8512 - acc: 0.7875 - val_loss: 0.8777 - val_acc: 0.7730\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8491 - acc: 0.7883 - val_loss: 0.8754 - val_acc: 0.7670\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8485 - acc: 0.7908 - val_loss: 0.8877 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7869 - val_loss: 0.8789 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8477 - acc: 0.7885 - val_loss: 0.8737 - val_acc: 0.7710\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7881 - val_loss: 0.8722 - val_acc: 0.7660\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8476 - acc: 0.7900 - val_loss: 0.8985 - val_acc: 0.7710\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.7865 - val_loss: 0.8861 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8490 - acc: 0.7897 - val_loss: 0.8863 - val_acc: 0.7690\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8481 - acc: 0.7872 - val_loss: 0.8863 - val_acc: 0.7790\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8472 - acc: 0.7884 - val_loss: 0.8747 - val_acc: 0.7760\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.7921 - val_loss: 0.8806 - val_acc: 0.7730\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8463 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7620\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8494 - acc: 0.7888 - val_loss: 0.8986 - val_acc: 0.7580\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.7880 - val_loss: 0.8724 - val_acc: 0.7720\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.7911 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.7901 - val_loss: 0.8803 - val_acc: 0.7680\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7919 - val_loss: 0.8756 - val_acc: 0.7760\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8457 - acc: 0.7896 - val_loss: 0.8704 - val_acc: 0.7780\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7897 - val_loss: 0.8741 - val_acc: 0.7680\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8451 - acc: 0.7889 - val_loss: 0.8812 - val_acc: 0.7730\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.7888 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8459 - acc: 0.7901 - val_loss: 0.8758 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8454 - acc: 0.7911 - val_loss: 0.8855 - val_acc: 0.7680\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7895 - val_loss: 0.8714 - val_acc: 0.7760\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8447 - acc: 0.7931 - val_loss: 0.8870 - val_acc: 0.7630\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.7917 - val_loss: 0.8808 - val_acc: 0.7760\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8469 - acc: 0.7899 - val_loss: 0.8759 - val_acc: 0.7670\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8457 - acc: 0.7880 - val_loss: 0.8780 - val_acc: 0.7730\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7921 - val_loss: 0.8760 - val_acc: 0.7710\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7892 - val_loss: 0.8776 - val_acc: 0.7700\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8449 - acc: 0.7893 - val_loss: 0.8746 - val_acc: 0.7680\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8450 - acc: 0.7915 - val_loss: 0.8837 - val_acc: 0.7720\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.7911 - val_loss: 0.8805 - val_acc: 0.7600\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7899 - val_loss: 0.8708 - val_acc: 0.7720\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8440 - acc: 0.7892 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8433 - acc: 0.7921 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8447 - acc: 0.7893 - val_loss: 0.8840 - val_acc: 0.7670\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8438 - acc: 0.7905 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8442 - acc: 0.7887 - val_loss: 0.8811 - val_acc: 0.7710\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8439 - acc: 0.7887 - val_loss: 0.8814 - val_acc: 0.7660\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8444 - acc: 0.7895 - val_loss: 0.9006 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8427 - acc: 0.7885 - val_loss: 0.8728 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8431 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7899 - val_loss: 0.8770 - val_acc: 0.7700\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8447 - acc: 0.7880 - val_loss: 0.8745 - val_acc: 0.7650\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8428 - acc: 0.7901 - val_loss: 0.8729 - val_acc: 0.7760\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8425 - acc: 0.7915 - val_loss: 0.8753 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7893 - val_loss: 0.8756 - val_acc: 0.7630\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8419 - acc: 0.7913 - val_loss: 0.8763 - val_acc: 0.7740\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8435 - acc: 0.7900 - val_loss: 0.8850 - val_acc: 0.7780\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8432 - acc: 0.7911 - val_loss: 0.8700 - val_acc: 0.7760\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8421 - acc: 0.7883 - val_loss: 0.8707 - val_acc: 0.7790\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.8699 - val_acc: 0.7750\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.7919 - val_loss: 0.8727 - val_acc: 0.7700\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8420 - acc: 0.7923 - val_loss: 0.8736 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8838 - val_acc: 0.7550\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8415 - acc: 0.7917 - val_loss: 0.8744 - val_acc: 0.7670\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8430 - acc: 0.7892 - val_loss: 0.8892 - val_acc: 0.7550\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8431 - acc: 0.7903 - val_loss: 0.8729 - val_acc: 0.7710\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.7931 - val_loss: 0.8717 - val_acc: 0.7700\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8413 - acc: 0.7903 - val_loss: 0.8742 - val_acc: 0.7740\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7908 - val_loss: 0.8823 - val_acc: 0.7750\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8423 - acc: 0.7908 - val_loss: 0.8700 - val_acc: 0.7700\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7907 - val_loss: 0.8698 - val_acc: 0.7720\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8405 - acc: 0.7913 - val_loss: 0.8691 - val_acc: 0.7740\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8429 - acc: 0.7896 - val_loss: 0.8698 - val_acc: 0.7740\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7912 - val_loss: 0.8722 - val_acc: 0.7700\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8410 - acc: 0.7901 - val_loss: 0.9618 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8452 - acc: 0.7873 - val_loss: 0.8821 - val_acc: 0.7790\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8439 - acc: 0.7900 - val_loss: 0.8696 - val_acc: 0.7820\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8402 - acc: 0.7912 - val_loss: 0.8697 - val_acc: 0.7610\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8730 - val_acc: 0.7640\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8392 - acc: 0.7928 - val_loss: 0.8805 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8410 - acc: 0.7911 - val_loss: 0.8692 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8393 - acc: 0.7912 - val_loss: 0.8701 - val_acc: 0.7720\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8404 - acc: 0.7904 - val_loss: 0.8670 - val_acc: 0.7790\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8417 - acc: 0.7921 - val_loss: 0.8851 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8400 - acc: 0.7905 - val_loss: 0.8999 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7907 - val_loss: 0.8827 - val_acc: 0.7700\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8418 - acc: 0.7880 - val_loss: 0.8833 - val_acc: 0.7710\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8380 - acc: 0.7939 - val_loss: 0.8869 - val_acc: 0.7600\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8395 - acc: 0.7927 - val_loss: 0.8714 - val_acc: 0.7600\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8386 - acc: 0.7903 - val_loss: 0.8788 - val_acc: 0.7680\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8409 - acc: 0.7909 - val_loss: 0.8762 - val_acc: 0.7700\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7908 - val_loss: 0.8687 - val_acc: 0.7720\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7929 - val_loss: 0.8948 - val_acc: 0.7550\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7925 - val_loss: 0.8860 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8376 - acc: 0.7923 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8389 - acc: 0.7901 - val_loss: 0.8712 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.7921 - val_loss: 0.8751 - val_acc: 0.7640\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8391 - acc: 0.7933 - val_loss: 0.8676 - val_acc: 0.7740\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8390 - acc: 0.7920 - val_loss: 0.8739 - val_acc: 0.7710\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8379 - acc: 0.7924 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8370 - acc: 0.7909 - val_loss: 0.8694 - val_acc: 0.7780\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8384 - acc: 0.7924 - val_loss: 0.8706 - val_acc: 0.7780\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7919 - val_loss: 0.8774 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8364 - acc: 0.7953 - val_loss: 0.8750 - val_acc: 0.7700\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8387 - acc: 0.7908 - val_loss: 0.8785 - val_acc: 0.7660\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8378 - acc: 0.7920 - val_loss: 0.8692 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8357 - acc: 0.7912 - val_loss: 0.8671 - val_acc: 0.7770\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8378 - acc: 0.7921 - val_loss: 0.8758 - val_acc: 0.7720\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8380 - acc: 0.7924 - val_loss: 0.8648 - val_acc: 0.7760\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8372 - acc: 0.7929 - val_loss: 0.8806 - val_acc: 0.7570\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8369 - acc: 0.7915 - val_loss: 0.8671 - val_acc: 0.7760\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8381 - acc: 0.7924 - val_loss: 0.8780 - val_acc: 0.7590\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8364 - acc: 0.7929 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8360 - acc: 0.7935 - val_loss: 0.8719 - val_acc: 0.7640\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8375 - acc: 0.7937 - val_loss: 0.8741 - val_acc: 0.7720\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8369 - acc: 0.7924 - val_loss: 0.8723 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.7937 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8361 - acc: 0.7883 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8349 - acc: 0.7940 - val_loss: 0.8714 - val_acc: 0.7690\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8356 - acc: 0.7939 - val_loss: 0.8732 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8354 - acc: 0.7935 - val_loss: 0.8716 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8361 - acc: 0.7916 - val_loss: 0.8665 - val_acc: 0.7740\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.8713 - val_acc: 0.7790\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8351 - acc: 0.7912 - val_loss: 0.8771 - val_acc: 0.7750\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8370 - acc: 0.7927 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7931 - val_loss: 0.8860 - val_acc: 0.7580\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.8719 - val_acc: 0.7750\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7935 - val_loss: 0.8811 - val_acc: 0.7600\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8350 - acc: 0.7932 - val_loss: 0.8670 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8361 - acc: 0.7917 - val_loss: 0.8775 - val_acc: 0.7670\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.7927 - val_loss: 0.8785 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7931 - val_loss: 0.8660 - val_acc: 0.7820\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8348 - acc: 0.7925 - val_loss: 0.8749 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8355 - acc: 0.7924 - val_loss: 0.8714 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8642 - val_acc: 0.7800\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8336 - acc: 0.7924 - val_loss: 0.8703 - val_acc: 0.7630\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8331 - acc: 0.7953 - val_loss: 0.8805 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8329 - acc: 0.7939 - val_loss: 0.8785 - val_acc: 0.7590\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8348 - acc: 0.7917 - val_loss: 0.8721 - val_acc: 0.7800\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8345 - acc: 0.7949 - val_loss: 0.9010 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7913 - val_loss: 0.8663 - val_acc: 0.7790\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8332 - acc: 0.7947 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8328 - acc: 0.7931 - val_loss: 0.8873 - val_acc: 0.7640\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8343 - acc: 0.7940 - val_loss: 0.8788 - val_acc: 0.7690\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8333 - acc: 0.7933 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8334 - acc: 0.7937 - val_loss: 0.8792 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7905 - val_loss: 0.8708 - val_acc: 0.7700\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8318 - acc: 0.7947 - val_loss: 0.8702 - val_acc: 0.7780\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.7961 - val_loss: 0.8727 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8334 - acc: 0.7940 - val_loss: 0.8657 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7917 - val_loss: 0.8748 - val_acc: 0.7620\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.7956 - val_loss: 0.8673 - val_acc: 0.7710\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8332 - acc: 0.7928 - val_loss: 0.8731 - val_acc: 0.7700\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.8649 - val_acc: 0.7750\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8340 - acc: 0.7935 - val_loss: 0.8692 - val_acc: 0.7730\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8743 - val_acc: 0.7750\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7935 - val_loss: 0.8815 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8325 - acc: 0.7924 - val_loss: 0.8768 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8309 - acc: 0.7928 - val_loss: 0.8755 - val_acc: 0.7800\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.7940 - val_loss: 0.8750 - val_acc: 0.7760\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7943 - val_loss: 0.8758 - val_acc: 0.7630\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8316 - acc: 0.7937 - val_loss: 0.8810 - val_acc: 0.7650\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.8650 - val_acc: 0.7790\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8313 - acc: 0.7931 - val_loss: 0.8739 - val_acc: 0.7790\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8301 - acc: 0.7935 - val_loss: 0.8809 - val_acc: 0.7660\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8326 - acc: 0.7952 - val_loss: 0.8729 - val_acc: 0.7660\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8318 - acc: 0.7915 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8314 - acc: 0.7937 - val_loss: 0.8660 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8322 - acc: 0.7944 - val_loss: 0.8653 - val_acc: 0.7800\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8303 - acc: 0.7969 - val_loss: 0.8620 - val_acc: 0.7780\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8307 - acc: 0.7940 - val_loss: 0.9117 - val_acc: 0.7530\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8319 - acc: 0.7949 - val_loss: 0.8722 - val_acc: 0.7640\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8318 - acc: 0.7956 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7940 - val_loss: 0.8882 - val_acc: 0.7560\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7953 - val_loss: 0.8652 - val_acc: 0.7710\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8320 - acc: 0.7960 - val_loss: 0.8838 - val_acc: 0.7720\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7943 - val_loss: 0.8648 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8288 - acc: 0.7957 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8293 - acc: 0.7955 - val_loss: 0.8815 - val_acc: 0.7700\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8303 - acc: 0.7967 - val_loss: 0.8624 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8296 - acc: 0.7955 - val_loss: 0.8666 - val_acc: 0.7700\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8304 - acc: 0.7936 - val_loss: 0.8820 - val_acc: 0.7740\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.7951 - val_loss: 0.8776 - val_acc: 0.7760\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8311 - acc: 0.7921 - val_loss: 0.9091 - val_acc: 0.7580\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8301 - acc: 0.7952 - val_loss: 0.8609 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8293 - acc: 0.7951 - val_loss: 0.8673 - val_acc: 0.7700\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8315 - acc: 0.7940 - val_loss: 0.8695 - val_acc: 0.7740\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7948 - val_loss: 0.8662 - val_acc: 0.7700\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7959 - val_loss: 0.8629 - val_acc: 0.7800\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8621 - val_acc: 0.7790\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8309 - acc: 0.7948 - val_loss: 0.8687 - val_acc: 0.7730\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8297 - acc: 0.7956 - val_loss: 0.8656 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8275 - acc: 0.7956 - val_loss: 0.8643 - val_acc: 0.7700\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7937 - val_loss: 0.8645 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8269 - acc: 0.7956 - val_loss: 0.8661 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7937 - val_loss: 0.8648 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8291 - acc: 0.7952 - val_loss: 0.8656 - val_acc: 0.7810\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8297 - acc: 0.7952 - val_loss: 0.8651 - val_acc: 0.7770\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8281 - acc: 0.7944 - val_loss: 0.8798 - val_acc: 0.7690\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.7987 - val_loss: 0.8627 - val_acc: 0.7750\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8275 - acc: 0.7961 - val_loss: 0.8654 - val_acc: 0.7710\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8281 - acc: 0.7956 - val_loss: 0.8720 - val_acc: 0.7670\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7943 - val_loss: 0.9056 - val_acc: 0.7610\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.7955 - val_loss: 0.8746 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8295 - acc: 0.7947 - val_loss: 0.8683 - val_acc: 0.7670\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8262 - acc: 0.7971 - val_loss: 0.8672 - val_acc: 0.7750\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8269 - acc: 0.7943 - val_loss: 0.8624 - val_acc: 0.7740\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7690\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.7949 - val_loss: 0.8687 - val_acc: 0.7670\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7971 - val_loss: 0.9091 - val_acc: 0.7640\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8273 - acc: 0.7960 - val_loss: 0.8981 - val_acc: 0.7720\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8323 - acc: 0.7923 - val_loss: 0.8765 - val_acc: 0.7760\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8276 - acc: 0.7931 - val_loss: 0.8668 - val_acc: 0.7680\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8286 - acc: 0.7952 - val_loss: 0.8701 - val_acc: 0.7690\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8278 - acc: 0.7963 - val_loss: 0.8693 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7953 - val_loss: 0.8649 - val_acc: 0.7690\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.7981 - val_loss: 0.8839 - val_acc: 0.7730\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.7940 - val_loss: 0.8716 - val_acc: 0.7770\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8268 - acc: 0.7971 - val_loss: 0.8685 - val_acc: 0.7830\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.7952 - val_loss: 0.8832 - val_acc: 0.7540\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8265 - acc: 0.7960 - val_loss: 0.8651 - val_acc: 0.7650\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8261 - acc: 0.7964 - val_loss: 0.8932 - val_acc: 0.7690\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8298 - acc: 0.7944 - val_loss: 0.8595 - val_acc: 0.7830\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8243 - acc: 0.7968 - val_loss: 0.8693 - val_acc: 0.7730\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8247 - acc: 0.7968 - val_loss: 0.8596 - val_acc: 0.7790\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.7988 - val_loss: 0.8680 - val_acc: 0.7660\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8262 - acc: 0.7965 - val_loss: 0.8655 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FPX9+PHXOyEhgXATrgQIcigQwimIBARFBMVbC7S2KCJfrVprtfX8idjaet/aeldbFDwqooKiCCooNwQMCATCEc4QJBASyPX+/TGTdQlLsoFsNpu8n4/HPrIz89nZ9+xM5j2fz8x8RlQVY4wxBiAs2AEYY4ypPiwpGGOM8bCkYIwxxsOSgjHGGA9LCsYYYzwsKRhjjPGwpFBNiEi4iOSISLvKLFvdich/ReRB9/1QEUn1p+xJfE+N+c1M1TuVbS/UWFI4Se4OpuRVLCJ5XsO/qej8VLVIVWNUdVtllj0ZInKmiKwQkUMi8pOIDA/E95SmqvNVtXtlzEtEFojItV7zDuhvVhuU/k29xncVkZkikiki+0Vktoh0DkKIphJYUjhJ7g4mRlVjgG3AxV7jppYuLyJ1qj7Kk/YSMBNoCFwI7AhuOOZERCRMRIL9f9wImAGcDrQEVgEfVWUA1fX/q5qsnwoJqWBDiYj8TUSmi8i7InIIuEZEBorIIhE5ICK7ROQ5EYlwy9cRERWRBHf4v+702e4R+w8i0qGiZd3po0Rkg4hki8jzIrLQ1xGfl0Jgqzo2q+q6cpZ1o4iM9BqOdI8Yk9x/ig9EZLe73PNFpOsJ5jNcRLZ4DfcVkVXuMr0L1PWa1kxEZrlHpz+LyCciEudOexQYCPzLrbk94+M3a+z+bpkiskVE7hERcadNFJFvRORpN+bNIjKijOW/3y1zSERSReSSUtP/z61xHRKRH0Wkpzu+vYjMcGPYJyLPuuP/JiL/9vp8JxFRr+EFIvJXEfkBOAy0c2Ne537HJhGZWCqGK9zf8qCIpInICBEZJyKLS5W7S0Q+ONGy+qKqi1T1DVXdr6oFwNNAdxFp5OO3ShaRHd47ShG5WkRWuO/PEqeWelBE9ojI476+s2RbEZF7RWQ38Ko7/hIRSXHX2wIRSfT6TD+v7WmaiLwvvzRdThSR+V5lj9leSn33Cbc9d/px66civ2ewWVIIrMuBd3COpKbj7GxvA5oDg4CRwP+V8flfA/8PaIpTG/lrRcuKSAvgPeDP7vemA/3LiXsJ8GTJzssP7wLjvIZHATtVdbU7/CnQGWgF/Aj8p7wZikhd4GPgDZxl+hi4zKtIGM6OoB3QHigAngVQ1buAH4Ab3ZrbH318xUtAPeA04FzgeuB3XtPPBtYAzXB2cq+XEe4GnPXZCHgYeEdEWrrLMQ64H/gNTs3rCmC/OEe2nwFpQALQFmc9+eu3wAR3nhnAHuAid/gG4HkRSXJjOBvnd7wDaAwMA7biHt3LsU091+DH+inHECBDVbN9TFuIs67O8Rr3a5z/E4DngcdVtSHQCSgrQcUDMTjbwO9F5EycbWIiznp7A/jYPUipi7O8r+FsTx9y7PZUESfc9ryUXj+hQ1XtdYovYAswvNS4vwFfl/O5O4H33fd1AAUS3OH/Av/yKnsJ8ONJlJ0AfOc1TYBdwLUniOkaYBlOs1EGkOSOHwUsPsFnzgCygSh3eDpw7wnKNndjr+8V+4Pu++HAFvf9ucB2QLw+u6SkrI/59gMyvYYXeC+j928GROAk6C5e028GvnLfTwR+8prW0P1scz+3hx+Bi9z3c4GbfZQZDOwGwn1M+xvwb6/hTs6/6jHL9kA5MXxa8r04Ce3xE5R7FZjivu8F7AMiTlD2mN/0BGXaATuBq8so8wjwivu+MZALxLvD3wMPAM3K+Z7hwBEgstSyTC5VbhNOwj4X2FZq2iKvbW8iMN/X9lJ6O/Vz2ytz/VTnl9UUAmu794CInCEin7lNKQeBh3B2kiey2+t9Ls5RUUXLtvGOQ52ttqwjl9uA51R1Fs6Oco57xHk28JWvD6jqTzj/fBeJSAwwGvfIT5yrfh5zm1cO4hwZQ9nLXRJ3hhtvia0lb0Skvoi8JiLb3Pl+7cc8S7QAwr3n576P8xou/XvCCX5/EbnWq8niAE6SLImlLc5vU1pbnARY5GfMpZXetkaLyGJxmu0OACP8iAHgLZxaDDgHBNPVaQKqMLdWOgd4VlXfL6PoO8CV4jSdXolzsFGyTV4HdAPWi8gSEbmwjPnsUdV8r+H2wF0l68H9HVrjrNc2HL/db+ck+LntndS8qwNLCoFVugval3GOIjupUz1+AOfIPZB24VSzARAR4didX2l1cI6iUdWPgbtwksE1wDNlfK6kCelyYJWqbnHH/w6n1nEuTvNKp5JQKhK3y7tt9i9AB6C/+1ueW6psWd3/7gWKcHYi3vOu8Al1ETkN+CdwE87RbWPgJ35Zvu1ARx8f3Q60F5FwH9MO4zRtlWjlo4z3OYZonGaWfwAt3Rjm+BEDqrrAnccgnPV3Uk1HItIMZzv5QFUfLausOs2Ku4ALOLbpCFVdr6pjcRL3k8CHIhJ1olmVGt6OU+tp7PWqp6rv4Xt7auv13p/fvER5256v2EKGJYWq1QCnmeWwOCdbyzqfUFk+BfqIyMVuO/ZtQGwZ5d8HHhSRHu7JwJ+AfCAaONE/JzhJYRQwCa9/cpxlPgpk4fzTPexn3AuAMBG5xT3pdzXQp9R8c4Gf3R3SA6U+vwfnfMFx3CPhD4C/i0iMOCflb8dpIqioGJwdQCZOzp2IU1Mo8RrwFxHpLY7OItIW55xHlhtDPRGJdnfM4Fy9c46ItBWRxsDd5cRQF4h0YygSkdHAeV7TXwcmisgwcU78x4vI6V7T/4OT2A6r6qJyvitCRKK8XhHuCeU5OM2l95fz+RLv4vzmA/E6byAivxWR5qpajPO/okCxn/N8BbhZnEuqxV23F4tIfZztKVxEbnK3pyuBvl6fTQGS3O0+GphcxveUt+2FNEsKVesOYDxwCKfWMD3QX6iqe4AxwFM4O6GOwEqcHbUvjwJv41ySuh+ndjAR55/4MxFpeILvycA5F3EWx54wfROnjXknkIrTZuxP3Edxah03AD/jnKCd4VXkKZyaR5Y7z9mlZvEMMM5tRnjKx1f8HifZpQPf4DSjvO1PbKXiXA08h3O+YxdOQljsNf1dnN90OnAQ+B/QRFULcZrZuuIc4W4DrnI/9jnOJZ1r3PnOLCeGAzg72I9w1tlVOAcDJdO/x/kdn8PZ0c7j2KPkt4FE/KslvALkeb1edb+vD07i8b5/p00Z83kH5wj7S1X92Wv8hcA6ca7YewIYU6qJ6IRUdTFOje2fONvMBpwarvf2dKM77VfALNz/A1VdC/wdmA+sB74t46vK2/ZCmhzbZGtqOre5Yidwlap+F+x4TPC5R9J7gURVTQ92PFVFRJYDz6jqqV5tVaNYTaEWEJGRItLIvSzv/+GcM1gS5LBM9XEzsLCmJwRxulFp6TYfXY9Tq5sT7Liqm2p5F6CpdMnAVJx251TgMrc6bWo5EcnAuc7+0mDHUgW64jTj1ce5GutKt3nVeLHmI2OMMR7WfGSMMcYj5JqPmjdvrgkJCcEOwxhjQsry5cv3qWpZl6MDIZgUEhISWLZsWbDDMMaYkCIiW8svZc1HxhhjvFhSMMYY42FJwRhjjIclBWOMMR6WFIwxxnhYUjDGGONhScEYY4yHJQVjjAkimeL7eVOqSn5RPoXFheWWrUwhd/OaMcYE09HCo6zes5r1WetpHdOazs06E10nmvQD6WzP3k6YhBEdEU29iHpE14kmqk4UhcWF5BXmsTtnN0t2LGHxjsXM3zKfVjGtaBrdlEFvDKJny54UazEpe1JYl7mOQ/mHKFbn+UIJjRNIaJzAzLFlPlqjUlhSMMbUSEcLj5KVl0XrmNY4T6F1jr7TD6QzL30eX2/5mv15+xncbjDJ7ZLZlr2Neenz2HZwGwPjBzI0YSi5Bbn8sP0HVu9dzb7cfWTlZpF+IP2Yo/eKigiLoFerXozpPoaGdRsSJmGszVzLO2veQUTo2bInv+nxG15a9hJ/G/Y3cgty2Zq9lfQD6Z4kEUgh10tqv3791Lq5MKZ2OVJ4hJ/zfiYmMoaYyBgKigv4fvv3fJ3+NVm5WQAUFBew9/BedufsZvvB7ew8tBOAlvVbMjRhKFF1opi3ZR7bsrd5xsfWj+XHvT96vqdJVBPaNWrHmr1rPDvgcAmnW2w3z1H99NTpfHD1B3SN7crunN1syNpAflE+CY0TaNfIeYx4XkEeeYV5nr91wuoQXSeakVNHkndfHlF1jn+yraoS9lAYOjkw+2QRWa6q/cotZ0nBGBMoRcVFpB9IJyc/B4AwCaNBZAMa1m1IeFg4BUUF5OTnsDZzLSl7Utids5uCogKOFh1l+8HtbDmwhR0Hd5BXmOeZZ3SdaADyCvMIl3CaRDcBnJ13i/otaBnTkrgGcXRo3IGm0U1ZvGMx87bMI78on3Pan8OwhGGc2+Fczmh+BiJC5uFMFmUsIr5hPD1b9ST8oXAO3HWA77d/T0xkDH3b9KVeRD2/l1mmSMB27KfCkoIxplIdPHqQHQd3kHEwg4yDGezO2Y2iRIRFECZhiAjFWszew3vJOJjB5p83s2bvGnILcv3+jkZ1GxEZHklkeCRtGrShQ5MOxDeIp3m95jSOakxOfg67c3ZTrMUM6zCMoQlDafRII792wt4765L3/u7AyytXXROBt2qRFERkJPAsEA68pqqPlJr+NDDMHawHtFDVxmXN05KCMYGTX5RPyu4Uth/czvbs7fy07ydS9qSQmpnKwaMH/ZpH3fC6xDWMo32j9iS1TCKpZRJNo5sCcPn0y3nz0jfJPpLNH7/4I8+Pep6oOlGc0fwMerToQaOoRp75VHRH62unfzLzqamCnhTcB8RvAM4HMoClwDhVXXuC8rcCvVV1QlnztaRgjP/yCvKYt2UeszbOYlv2Nk87el5BHvuP7KewuJB2DdvRtlFbluxYwsfrP+bAkQOezzeOakxSyyR6tOhB+0btiW8YT1zDOOIbxtM6pjX1/l6PnHtyKNIiAASh4SMNT3on7GsHfipH+KeqJiWU6pAUBgIPquoF7vA9AKr6jxOU/x6YrKpfljVfSwrG/CK/KJ/NP29mY9ZG1metZ/We1azes5qUPSnERMaQW5BLsRZTL6IeHZt0JDM3k8zDmdSLqEfT6KZszd5KnbA6nqtpxvccz+guo+nUtBPxDeOJfTzWr52wvztzfz9rKl91SApXASNVdaI7/FtggKre4qNse2AREK/qHnIcO30SMAmgXbt2fbdu9etZEcaEjMLiQnYd2sWWA1vYkLWB9APpPPzdw7x44YvkF+WzYtcKVu1eBUDT6KZ8s/UbTmtyGlsObDnmMsU2DdqQ1DKJdg3bUT+yPjGRMfz127+Sd18e0Q9Ho5P1uKtcioqL2J2zm9j6sUSGR5Ybq+3EQ1N1SApXAxeUSgr9VfVWH2XvwkkIx00rzWoKpro6UniEFbtWEFsvlk5NOyEiFBQVsDZzLRkHM9ift5/fzfgdE3pNYO2+tezL3UdBUQFHCo+QmZt5zM49TMKOGW4V04o+rfswa+MsBrcbzNGio5zW5DQ6N+3svJo5f5vVaxaMRTchwN+kEMib1zKAtl7D8cDOE5QdC9wcwFiMqVTFWsynGz7l0mmXcmv/W3l+yfPUDa/L0aKjADSLbkZWXtYx40p8tvEzusV2I21/GuN7jicyPJJXV7zKy6Nfpl2jdnRp1oV2jdpRrMXsz9tP6ydbs+uOXcFYTFMLBbKmUAfnRPN5wA6cE82/VtXUUuVOB74AOqgfwVhNwVS1ouIijhQeoX5kfcBpPhmaMJT5W+YD0LBuQzo17cQ57c9hSPsh7Mvdx+KMxWw7uI0eLXrQt3VfTmtyGs3qNSO2XuwxV9gYU1WCXlNQ1UIRuQVnhx8OvKGqqSLyELBMVUs68RgHTPMnIRgTSNlHsvl267d8v/17UjNTWbdvHWn7044pM6T9EOpH1Gf5zuW8evGr/Kr7r2hYt+ExZazN3YQyu3nN1BpFxUWEh4UfMy4rN4v3Ut9j6pqp/JDxA8VaTERYBF2adaFbbDfiG8Z7bqja9PMm1maupU2DNjx1wVOeLg2MCQVBrykYU11s2r+J62dez8LtC+kW243erXrzVspbJLVMYl3mOgqKC0hskch9g+9jWMIwBrYd6LNvGmNqA0sKpkYpKCrgjZVvsGbvGuIaxFFQXMCjCx8lIiyC3/f7PRv2b+CLTV94Tuau3rOaVf+3iqSWSYQ9FMZDwx6y5h9Tq1lSMDWCqjJr4yzu/PJOftr3Ew0iG3Ao/5Bn+vbbt9P26bZl7uxLpllCMLWZJQUT8lbvWc0dc+7gq81f0aVZF2aOncnoLqPJK8xjf95+4hrEIWJH/8b4w5KCCTkFRQXMTpvNyl0rWb5rOZ9t/IzGUY155oJnuOnMm4gMj/Q0AVWky2NjjCUFE2JkipDcLpkF2xYgCJ2aduL2s27nvsH30SS6iScZWK3AmJMTFuwAjPHXkh1LiGsQx/Kdy3nrsrc4dM8hNty6gSd/eNLzoBVLBsacGksKJiSs2LWCc986l4jwCH64/gfGzxjvucPYEoExlceaj0y1tz17O6PfGU2zes34fsL3tG7Q2hKBMQFiNQVTbckU4ad9P9HumXYcLjjMZ7/+jNYNWgc7LGNqNEsKplrJK8jj9RWvM+C1AUSGR9L1xa6ESzjvX/0+iS0Sgx2eMTWeNR+ZauOtVW9x7cfXAtCjRQ9uG3Ab3WO7M7DtQLo06xLc4IypJSwpmGpBpggAg9sNZsrQKQxNGIqIBDkqY2ofSwom6J5b/BwAF3e5mPevfp+6deoGOSJjai9LCiZocvJzuG32bbyx6g0uP+Nypl01za9nBBtjAsdONJsqJVOErNwsPlz7Ib3+1Ys3V73Jvcn3Mv2q6ZYQjKkGrKZgqoRMEZ44/wkSWyTS/PHmnvHfXvstg9sPDmJkxhhvlhRMwMgUYcaYGXyy4RPqR9Tnzi/vZFDbQTx87sMMbjeY/nH97fyBMdWMJQVTYb4eQuM9TqYID5/7MJHhkVw2/TIaRDbgqm5XcduA2+jduncwQjbG+MmSgqkwX11MlIw7cOQAl5x+Cfd9fR9Xdr2Sm8+8meR2yUSER1R1mMaYk2BJwfitdA1hzZ41JP0riVcvfpVt2dtYvms5izIWcfDoQZ4d+Sy39r/V7jUwJsSIamh1LNavXz9dtmxZsMOo1Yq1mMnzJvO37/7mGRcmYXRt3pW+bfpyY98bGdh2YBAjNMaUJiLLVbVfeeWspmCOU7pGUFBUwLdbv6VIi4iqE8WjCx9l1sZZTOg1gVsH3ErT6KbE1oslOiI6iFEbYyqDJQVznJKEsCdnD62ebEXrmNbsytnlmV4nrA4vXfgSN/a70ZqHjKlhLCnUciW1gsLiQsIkjDBx7mecv2U+v3r/VwhCr1a9eKnvS8TWiyWvMI/2jdrTuVnnIEdujAkESwq1nE5WdufsZsibQziUf4iru11N83rNeeibh+jcrDNfj//auqw2phYJaDcXIjJSRNaLSJqI3H2CMr8SkbUikioi7wQyntqupCdSbweOHGDkf0ey49AOBsQN4JXlrzB5/mRGdxnN4omLLSEYU8sErKYgIuHAi8D5QAawVERmquparzKdgXuAQar6s4i0CFQ8tYWvG8tKFP6/Qv67+r88tvAx1uxdw12D7mLh9oWszVzLJ+M+YeTUkWTfnc36fevp26avpynJGFN7BPK/vj+QpqqbVTUfmAZcWqrMDcCLqvozgKruDWA8tYL3XcUlZIqwcNtCer3ci99+9FvCJIxBbQfx5A9PsnDbQt6+/G0u6HQBOllpWLchZ8adaQnBmFoqkOcU4oDtXsMZwIBSZboAiMhCIBx4UFU/Lz0jEZkETAJo165dQIKtabxrC6+MfoVhbw0jvmE8066cxtXdryZMwsjJz+Fw/mFaxrQMYqTGmOokkIeDvq5VLN2uUQfoDAwFxgGviUjj4z6k+oqq9lPVfrGxsZUeaKjzda4A4GjhUW6ZdQuTPp3EuR3OZcX/rWBM4hhPLSAmMsYSgjHmGIGsKWQAbb2G44GdPsosUtUCIF1E1uMkiaUBjKvG8XUOYUPWBsZ+MJaVu1dyx8A7eHT4o4SHhQchOmNMKAlkTWEp0FlEOohIJDAWmFmqzAxgGICINMdpTtocwJhqhbdT3qbPy33Ymr2VGWNm8MSIJywhGGP8ErCagqoWisgtwBc45wveUNVUEXkIWKaqM91pI0RkLVAE/FlVswIVU02XV5DHH2b/gddWvsY57c9h6hVTiWsYF+ywjDEhxDrEqyF+2vcT4z4cx6rdq7g3+V6mDJtCnTC7N9EY47AO8WoJVeXl5S/zpy/+RHRENJ+M+4TRXUYHOyxjTIiyi9FDTMmVRjJFOFp4lLEfjuWmz25icPvBrLlpjSUEY8wpsZpCiCm50ijnnhwumXYJczbN4dHhj3Ln2XfaDWfGmFNme5EQUPo+hOwj2YycOpKvNn/FG5e8wV8G/cUSgjGmUlhNIQR434dw4MgBRvxnBCt3r/TcnWyMMZXFkkII2Z+3n/P/cz5r9qzhw199yCWnXxLskIwxNYwlhRChqlz87sWk7k1lxtgZXNj5wmCHZIypgawhupo5UT9GH/30Ed9v/56XLnrJEoIxJmAsKVQz3ucPShJEUXERD8x7gDOan8H4nuODFZoxphaw5qNqrCRBTE+dTmpmKtOunGZ9GBljAspqCtVcYXEhk+dPpkeLHnalkTEm4KymUA0VazGXTruUlbtWcuDIAQ4XHGbGmBl2L4IxJuAsKVRDczfP5dMNn3Jxl4vp2KQj3Vt0t8tPjTFVwpJCNSFTxHMO4fWVr9M0uinvXf0eUXWighyZMaY2sfaIaqIkIezL3cdHP33Eb5N+awnBGFPlLClUM/9d/V/yi/K5vvf1wQ7FGFMLWVIIotI3qqkqr614jQFxA+jRskeQojLG1GaWFILI+0Y1gMU7FpOamcrEPhODFJExprazpFCNvLj0RepH1GdM9zHBDsUYU0tZUqgmth7Yyrtr3mVS30k0qNsg2OEYY2opSwrVxNOLnkZEuP2s24MdijGmFrOkUA1k5Wbx6opX+XWPX9O2Udtgh2OMqcUsKVQDLy19idyCXP589p+DHYoxppazpBBkuQW5PL/keS7qfBGJLRKDHY4xppazpBBkr614jczcTO4adFewQzHGmMAmBREZKSLrRSRNRO72Mf1aEckUkVXuq1ZdoH+08CiPLXyMIe2HMLj94GCHY4wxgUsKIhIOvAiMAroB40Skm4+i01W1l/t6LVDxVCcldzK/nfI2Ow7t4L7B9wU5ImOMcQSyptAfSFPVzaqaD0wDLg3g94UMnawUFhfyjwX/4Mw2Z3L+aecHOyRjjAECmxTigO1ewxnuuNKuFJHVIvKBiNSa6zHfXfMu6QfSuW/wfYhI+R8wxpgqEMik4GtPp6WGPwESVDUJ+Ap4y+eMRCaJyDIRWZaZmVnJYQbH6ytf54zmZ3Dx6RcHOxRjjPEIZFLIALyP/OOBnd4FVDVLVY+6g68CfX3NSFVfUdV+qtovNjY2IMFWpQNHDrBg2wIuP+Nye8SmMaZaCeQeaSnQWUQ6iEgkMBaY6V1ARFp7DV4CrAtgPNXGF2lfUKRFjO4yOtihGGPMMQL2OE5VLRSRW4AvgHDgDVVNFZGHgGWqOhP4g4hcAhQC+4FrAxVPdfLZxs9oFt2MAXEDgh2KMcYcI6DPaFbVWcCsUuMe8Hp/D3BPIGOoboqKi5i1cRajOo8iPCw82OEYY8wxrEG7ii3ZsYSsvCwu6nxRsEMxxpjjWFKoYp9u+JRwCeeCjhcEOxRjjDmOJYUqUnIX82cbP2NQu0E0iW4S5IiMMeZ4lhQCqCQRgHMXc9r+NFL2pDC6s111ZIypniwpBJBO/uVevaLiIiZ8PIEGkQ0Ymzg2iFEZY8yJBfTqI/OLfyz4B99t+463L3vbnq5mjKm2rKZQBRZlLOLB+Q/y6x6/5pqka4IdjjHGnJAlhSpw6+xbiW8Yz0sXvmSd3xljqjW/koKIdBSRuu77oSLyBxFpHNjQaoa0/Wks27mMW/vfSqOoRsEOxxhjyuRvTeFDoEhEOgGvAx2AdwIWVQ0y/cfpAPyq+6+CHIkxxpTP36RQrKqFwOXAM6p6O9C6nM8YYHrqdM5ue7adXDbGhAR/k0KBiIwDxgOfuuMiAhNSzbEucx1r9q5hTPcxwQ7FGGP84m9SuA4YCDysquki0gH4b+DCCm0lN61NT52OIFzV7aogR2SMMf7x6z4FVV0L/AFARJoADVT1kUAGFsp0sqKqvJf6HkPaD6FNgzbBDskYY/zi79VH80WkoYg0BVKAN0XkqcCGFtp+3Psj6/ats6YjY0xI8bf5qJGqHgSuAN5U1b7A8MCFFfo+3eCcerm86+VBjsQYY/znb1Ko4z4681f8cqLZlGHO5jn0atWLVjGtgh2KMcb4zd+k8BDOYzU3qepSETkN2Bi4sEJbTn4OC7ctZMRpI4IdijHGVIi/J5rfB973Gt4MXBmooELdN1u+oaC4gBEdLSkYY0KLvyea40XkIxHZKyJ7RORDEYkPdHChas6mOUTXiWZQu0HBDsUYYyrE3+ajN4GZQBsgDvjEHWd8+GLTFwxNGEpUnahgh2KMMRXib1KIVdU3VbXQff0biA1gXCFr64GtrM9ab01HxpiQ5G9S2Cci14hIuPu6BsgKZGCh6svNXwJYUjDGhCR/k8IEnMtRdwO7gKtwur4wrpKuLeZsmkNcgzi6Nu8a5IiMMabi/EoKqrpNVS9R1VhVbaGql+HcyGZcOlnJPpLN7LTZjOw00h6mY4wJSafy5LU/VVoUNcTLy18mJz+Hm8+8OdihGGPMSTmVpFDuobCIjBSR9SKSJiJ3l1HuKhFREel3CvEEVX5RPs8ufpbzOpxH79a9gx2OMcaclFNJClrWRBEJB14ERgHdgHEi0s1HuQY4PbAuPoVYgu7dNe+y89BO/nz2n4MdijHGnLQyk4KIHBKRgz5eh3DuWShLfyBNVTeraj6E7M+rAAAZAUlEQVQwDbjUR7m/Ao8BR05mAaoDVeWJH54gsUWiXXVkjAlpZSYFVW2gqg19vBqoanldZMQB272GM9xxHiLSG2irqmV2sicik0RkmYgsy8zMLOdrq96cTXP4ce+P3DnwTjvBbIwJaafSfFQeX3tHT5OTiIQBTwN3lDcjVX1FVfupar/Y2Op3z9w7P75Dk6gmjOsxLtihGGPMKQlkUsgAvJ9WHw/s9BpuACQC80VkC3AWMDPUTjYXFhfyyfpPGN1lNJHhkcEOxxhjTolfvaSepKVAZ/d5zjuAscCvSyaqajbQvGRYROYDd6rqsgDGVOki/hoBwGVnXBbkSIwx5tQFrKagqoXALTjPYVgHvKeqqSLykIhcEqjvrWp/6P8HoupEcUHHC4IdijHGnLJA1hRQ1VnArFLjHjhB2aGBjCUQVJUZ62cwouMI6kfWD3Y4xhhzygJ5TqHGW7V7Fduyt3HZ6dZ0ZIypGSwpnIIZP80gTMIY3WV0sEMxxphKYUnhFHz000ckt0smtn71u0zWGGNOhiWFk7R6z2rW7F3DlV3tUdXGmJrDksJJenX5q9QNr8s1SdcEOxRjjKk0lhROQm5BLv9Z/R+u7HYlTaObBjscY4ypNJYUTsIHaz8g+2g2k/pMCnYoxhhTqSwpnITxM8bTpVkXhrQfEuxQjDGmUllSqKC1mWsBmNh7ovWIaoypcSwpVNC/V/2bOmF1GN9rfLBDMcaYSmdJoQJUlY9++ojzOpxHi/otgh2OMcZUOksKFbA+az1p+9O45PQa05+fMcYcw5JCBXR9sSsAF3e5OMiRGGNMYAS0l9SaJrldMofzD9O2UdvyCxtjTAiymoKfMg9n8v32763pyBhTo1lS8NOsjbMo1mJLCsaYGs2Sgp9mbphJXIM4erfqHexQjDEmYCwp+OFI4RG+SPuCi7tcbDesGWNqNEsK5ZApwsJtCzlccJiLulwU7HCMMSagLCmUQycrczbNISIsgqEJQ4MdjjHGBJQlBT/M2TyHQe0GERMZE+xQjDEmoCwplGNPzh5W7V7FiNNGBDsUY4wJOEsK5ZibPheA8zueH+RIjDEm8CwplGPOpjk0i25ml6IaY2oFSwplUHVOMg8/bTjhYeHBDscYYwLOkkIZUjNT2ZWzixEd7XyCMaZ2CGhSEJGRIrJeRNJE5G4f028UkTUiskpEFohIt0DGUxEyRfhy05cAnH+anU8wxtQOAUsKIhIOvAiMAroB43zs9N9R1R6q2gt4DHgqUPFUlE5W5qbP5fRmp1uvqMaYWiOQNYX+QJqqblbVfGAacKl3AVU96DVYH9AAxlMhxVrMwu0LGdJ+SLBDMcaYKhPI5ynEAdu9hjOAAaULicjNwJ+ASOBcXzMSkUnAJIB27dpVeqC+rM1cy4EjBxjUdlCVfJ8xxlQHgawp+Oo57riagKq+qKodgbuA+33NSFVfUdV+qtovNja2ksP0beG2hYDzYB1jjKktApkUMgDvxvh4YGcZ5acBlwUwngpZsH0BLeu35LQmpwU7FGOMqTKBTApLgc4i0kFEIoGxwEzvAiLS2WvwImBjAOOpkIXbFpLcLtm6yjbG1CoBSwqqWgjcAnwBrAPeU9VUEXlIREoeX3aLiKSKyCqc8wrjAxVPRew8tJP0A+l2PsEYU+sE8kQzqjoLmFVq3ANe728L5PefrJLzCYPaWVIwxtQudkezDwu3LyS6TrT1d2SMqXUsKfiwYNsCBsQPICI8ItihGGNMlbKk4EWmCDn5OazavYrktnYpqjGm9rGk4EUnK4syFlGkRXY+wRhTK1lSKGVe+jzCJdyuPDLG1EqWFEqZt2UeZ8adSYO6DYIdijHGVDlLCl5y8nNYunMpwxKGBTsUY4wJCksKXhZuW0hhcSFDE4YGOxRjjAkKSwpe5m2ZR0RYhJ1PMMbUWpYUvMzfMp/+cf2pH1k/2KEYY0xQWFJwHTp6iGU7l1nTkTGmVrOk4Ppu23cUaZGdZDbG1GqWFFzzt8wnMjySgW0HBjsUY4wJGksKrq/Tv2ZA3ADqRdQLdijGGBM0lhSAfbn7WLFrBeefdn6wQzHGmKCypAB8tfkrFOWCThcEOxRjjAmqWp8UZIowZ9McmkQ1oW/rvsEOxxhjgiqgT14LBcUPFNP26bYMP2044WHhwQ7HmIAqKCggIyODI0eOBDsUEyBRUVHEx8cTEXFyz4Op9Ulh3b517Di0gxEdRwQ7FGMCLiMjgwYNGpCQkICIBDscU8lUlaysLDIyMujQocNJzaPWNx/N2TQHwE4ym1rhyJEjNGvWzBJCDSUiNGvW7JRqgpYUNs3h9Gan075x+2CHYkyVsIRQs53q+q21SUGmCEcLjzJ/y3xrOjLGGFetTQo6WVmwbQF5hXmWFIypIllZWfTq1YtevXrRqlUr4uLiPMP5+fl+zeO6665j/fr1ZZZ58cUXmTp1amWEXOnuv/9+nnnmmePGjx8/ntjYWHr16hWEqH5Rq080f572ORFhEdYJnjFVpFmzZqxatQqABx98kJiYGO68885jyqgqqkpYmO9j1jfffLPc77n55ptPPdgqNmHCBG6++WYmTZoU1Dhqd1LY9DmD2w8mJjIm2KEYU+X++PkfWbV7VaXOs1erXjwz8vij4PKkpaVx2WWXkZyczOLFi/n000+ZMmUKK1asIC8vjzFjxvDAAw8AkJyczAsvvEBiYiLNmzfnxhtvZPbs2dSrV4+PP/6YFi1acP/999O8eXP++Mc/kpycTHJyMl9//TXZ2dm8+eabnH322Rw+fJjf/e53pKWl0a1bNzZu3Mhrr7123JH65MmTmTVrFnl5eSQnJ/PPf/4TEWHDhg3ceOONZGVlER4ezv/+9z8SEhL4+9//zrvvvktYWBijR4/m4Ycf9us3OOecc0hLS6vwb1fZam3zUcbBDH7c+yOjOo0KdijGGGDt2rVcf/31rFy5kri4OB555BGWLVtGSkoKX375JWvXrj3uM9nZ2ZxzzjmkpKQwcOBA3njjDZ/zVlWWLFnC448/zkMPPQTA888/T6tWrUhJSeHuu+9m5cqVPj972223sXTpUtasWUN2djaff/45AOPGjeP2228nJSWF77//nhYtWvDJJ58we/ZslixZQkpKCnfccUcl/TpVJ6A1BREZCTwLhAOvqeojpab/CZgIFAKZwARV3RrImEp8kfYFACM7jayKrzOm2jmZI/pA6tixI2eeeaZn+N133+X111+nsLCQnTt3snbtWrp163bMZ6Kjoxk1yjmw69u3L999953PeV9xxRWeMlu2bAFgwYIF3HXXXQD07NmT7t27+/zs3Llzefzxxzly5Aj79u2jb9++nHXWWezbt4+LL74YcG4YA/jqq6+YMGEC0dHRADRt2vRkfoqgClhSEJFw4EXgfCADWCoiM1XVO92vBPqpaq6I3AQ8BowJVEzeZqfNJq5BHN1jfW8IxpiqVb/+L0883LhxI88++yxLliyhcePGXHPNNT6vvY+MjPS8Dw8Pp7Cw0Oe869ate1wZVS03ptzcXG655RZWrFhBXFwc999/vycOX5d+qmrIX/IbyOaj/kCaqm5W1XxgGnCpdwFVnaeque7gIiA+gPF4FBQV8OXmLxnVaVTIr0BjaqKDBw/SoEEDGjZsyK5du/jiiy8q/TuSk5N57733AFizZo3P5qm8vDzCwsJo3rw5hw4d4sMPPwSgSZMmNG/enE8++QRwbgrMzc1lxIgRvP766+Tl5QGwf//+So870AKZFOKA7V7DGe64E7kemO1rgohMEpFlIrIsMzPzlANbvGMxB48etKYjY6qpPn360K1bNxITE7nhhhsYNGhQpX/Hrbfeyo4dO0hKSuLJJ58kMTGRRo0aHVOmWbNmjB8/nsTERC6//HIGDBjgmTZ16lSefPJJkpKSSE5OJjMzk9GjRzNy5Ej69etHr169ePrpp31+94MPPkh8fDzx8fEkJCQAcPXVVzN48GDWrl1LfHw8//73vyt9mf0h/lShTmrGIlcDF6jqRHf4t0B/Vb3VR9lrgFuAc1T1aFnz7devny5btuyUYrtv7n08uvBR9v1lH42jGp/SvIwJJevWraNr167BDqNaKCwspLCwkKioKDZu3MiIESPYuHEjdeqE/kWZvtaziCxX1X7lfTaQS58BtPUajgd2li4kIsOB+/AjIVSWzzd9ztltz7aEYEwtlpOTw3nnnUdhYSGqyssvv1wjEsKpCuQvsBToLCIdgB3AWODX3gVEpDfwMjBSVfcGMBaP3IJcVu1exb3J91bF1xljqqnGjRuzfPnyYIdR7QTsnIKqFuI0CX0BrAPeU9VUEXlIRC5xiz0OxADvi8gqEZkZqHhKrN6zmmItpm8be6COMcaUFtC6kqrOAmaVGveA1/vhgfz+0mSK8MKoFwDsKWvGGONDrbqjWScrK3atoHm95sQ3rJKrX40xJqTUqqQAsHzXcvq27mv3JxhjjA+1KikcKTxCamaqNR0ZEyRDhw497ka0Z555ht///vdlfi4mxum0cufOnVx11VUnnHd5l6s/88wz5ObmeoYvvPBCDhw44E/oVWr+/PmMHj36uPEvvPACnTp1QkTYt29fQL67ViWFNXvWUFhcSJ/WfYIdijG10rhx45g2bdox46ZNm8a4ceP8+nybNm344IMPTvr7SyeFWbNm0bhx6FyaPmjQIL766ivatw/ckyJrVVJYsWsFgF15ZEwFyZTKaW696qqr+PTTTzl61LklacuWLezcuZPk5GTPfQN9+vShR48efPzxx8d9fsuWLSQmJgJOFxRjx44lKSmJMWPGeLqWALjpppvo168f3bt3Z/LkyQA899xz7Ny5k2HDhjFs2DAAEhISPEfcTz31FImJiSQmJnoegrNlyxa6du3KDTfcQPfu3RkxYsQx31Pik08+YcCAAfTu3Zvhw4ezZ88ewLkX4rrrrqNHjx4kJSV5usn4/PPP6dOnDz179uS8887z+/fr3bu35w7ogCl5oEWovPr27asn64aZN2iTR5pocXHxSc/DmFC2du3aYIegF154oc6YMUNVVf/xj3/onXfeqaqqBQUFmp2draqqmZmZ2rFjR8//av369VVVNT09Xbt3766qqk8++aRed911qqqakpKi4eHhunTpUlVVzcrKUlXVwsJCPeecczQlJUVVVdu3b6+ZmZmeWEqGly1bpomJiZqTk6OHDh3Sbt266YoVKzQ9PV3Dw8N15cqVqqp69dVX63/+85/jlmn//v2eWF999VX905/+pKqqf/nLX/S22247ptzevXs1Pj5eN2/efEys3ubNm6cXXXTRCX/D0stRmq/1DCxTP/axta6m0LeNnWQ2Jpi8m5C8m45UlXvvvZekpCSGDx/Ojh07PEfcvnz77bdcc801ACQlJZGUlOSZ9t5779GnTx969+5Namqqz87uvC1YsIDLL7+c+vXrExMTwxVXXOHphrtDhw6eB+94d73tLSMjgwsuuIAePXrw+OOPk5qaCjhdaXs/Ba5JkyYsWrSIIUOG0KFDB6D6da9da5JCflE+a/auoU8rO59gTDBddtllzJ071/NUtT59nP/JqVOnkpmZyfLly1m1ahUtW7b02V22N18HeOnp6TzxxBPMnTuX1atXc9FFF5U7Hy2jD7iSbrfhxN1z33rrrdxyyy2sWbOGl19+2fN96qMrbV/jqpNakxRS96aSX5Rv5xOMCbKYmBiGDh3KhAkTjjnBnJ2dTYsWLYiIiGDevHls3Vr287aGDBnC1KlTAfjxxx9ZvXo14HS7Xb9+fRo1asSePXuYPfuXzpcbNGjAoUOHfM5rxowZ5ObmcvjwYT766CMGDx7s9zJlZ2cTF+d0Av3WW295xo8YMYIXXnjBM/zzzz8zcOBAvvnmG9LT04Hq1712rUkKy3c5fZzY5ajGBN+4ceNISUlh7NixnnG/+c1vWLZsGf369WPq1KmcccYZZc7jpptuIicnh6SkJB577DH69+8POE9R6927N927d2fChAnHdLs9adIkRo0a5TnRXKJPnz5ce+219O/fnwEDBjBx4kR69+7t9/I8+OCDnq6vmzdv7hl///338/PPP5OYmEjPnj2ZN28esbGxvPLKK1xxxRX07NmTMWN8P1ds7ty5nu614+Pj+eGHH3juueeIj48nIyODpKQkJk6c6HeM/gpY19mBcrJdZ3/808e8uepNPhrzUbWuuhkTSNZ1du1QXbvOrlYuPeNSLj3j0vILGmNMLVZrmo+MMcaUz5KCMbVMqDUZm4o51fVrScGYWiQqKoqsrCxLDDWUqpKVlUVUVNRJz6PWnFMwxuC5ciUzMzPYoZgAiYqKIj7+5B8NYEnBmFokIiLCcyetMb5Y85ExxhgPSwrGGGM8LCkYY4zxCLk7mkUkEyi7U5TjNQcC85iiqmfLUj3ZslRfNWl5TmVZ2qtqbHmFQi4pnAwRWebP7d2hwJalerJlqb5q0vJUxbJY85ExxhgPSwrGGGM8aktSeCXYAVQiW5bqyZal+qpJyxPwZakV5xSMMcb4p7bUFIwxxvjBkoIxxhiPGp0URGSkiKwXkTQRuTvY8VSEiLQVkXkisk5EUkXkNnd8UxH5UkQ2un+bBDtWf4lIuIisFJFP3eEOIrLYXZbpIhIZ7Bj9JSKNReQDEfnJXUcDQ3XdiMjt7jb2o4i8KyJRobJuROQNEdkrIj96jfO5HsTxnLs/WC0ifYIX+fFOsCyPu9vYahH5SEQae027x12W9SJyQWXFUWOTgoiEAy8Co4BuwDgR6RbcqCqkELhDVbsCZwE3u/HfDcxV1c7AXHc4VNwGrPMafhR42l2Wn4HrgxLVyXkW+FxVzwB64ixXyK0bEYkD/gD0U9VEIBwYS+ism38DI0uNO9F6GAV0dl+TgH9WUYz++jfHL8uXQKKqJgEbgHsA3H3BWKC7+5mX3H3eKauxSQHoD6Sp6mZVzQemASHzPE5V3aWqK9z3h3B2OnE4y/CWW+wt4LLgRFgxIhIPXAS85g4LcC7wgVsklJalITAEeB1AVfNV9QAhum5wekuOFpE6QD1gFyGyblT1W2B/qdEnWg+XAm+rYxHQWERaV02k5fO1LKo6R1UL3cFFQEmf2JcC01T1qKqmA2k4+7xTVpOTQhyw3Ws4wx0XckQkAegNLAZaquoucBIH0CJ4kVXIM8BfgGJ3uBlwwGuDD6X1cxqQCbzpNoe9JiL1CcF1o6o7gCeAbTjJIBtYTuiuGzjxegj1fcIEYLb7PmDLUpOTgvgYF3LX34pIDPAh8EdVPRjseE6GiIwG9qrqcu/RPoqGyvqpA/QB/qmqvYHDhEBTkS9ue/ulQAegDVAfp5mltFBZN2UJ2W1ORO7DaVKeWjLKR7FKWZaanBQygLZew/HAziDFclJEJAInIUxV1f+5o/eUVHndv3uDFV8FDAIuEZEtOM145+LUHBq7TRYQWusnA8hQ1cXu8Ac4SSIU181wIF1VM1W1APgfcDahu27gxOshJPcJIjIeGA38Rn+5sSxgy1KTk8JSoLN7FUUkzkmZmUGOyW9um/vrwDpVfcpr0kxgvPt+PPBxVcdWUap6j6rGq2oCznr4WlV/A8wDrnKLhcSyAKjqbmC7iJzujjoPWEsIrhucZqOzRKSeu82VLEtIrhvXidbDTOB37lVIZwHZJc1M1ZWIjATuAi5R1VyvSTOBsSJSV0Q64Jw8X1IpX6qqNfYFXIhzxn4TcF+w46lg7Mk41cHVwCr3dSFOW/xcYKP7t2mwY63gcg0FPnXfn+ZuyGnA+0DdYMdXgeXoBSxz188MoEmorhtgCvAT8CPwH6BuqKwb4F2ccyEFOEfP159oPeA0ubzo7g/W4FxxFfRlKGdZ0nDOHZTsA/7lVf4+d1nWA6MqKw7r5sIYY4xHTW4+MsYYU0GWFIwxxnhYUjDGGONhScEYY4yHJQVjjDEelhSMcYlIkYis8npV2l3KIpLg3fulMdVVnfKLGFNr5Klqr2AHYUwwWU3BmHKIyBYReVRElrivTu749iIy1+3rfq6ItHPHt3T7vk9xX2e7swoXkVfdZxfMEZFot/wfRGStO59pQVpMYwBLCsZ4iy7VfDTGa9pBVe0PvIDTbxPu+7fV6et+KvCcO/454BtV7YnTJ1KqO74z8KKqdgcOAFe64+8GervzuTFQC2eMP+yOZmNcIpKjqjE+xm8BzlXVzW4nhbtVtZmI7ANaq2qBO36XqjYXkUwgXlWPes0jAfhSnQe/ICJ3ARGq+jcR+RzIwekuY4aq5gR4UY05IaspGOMfPcH7E5Xx5ajX+yJ+Oad3EU6fPH2B5V69kxpT5SwpGOOfMV5/f3Dff4/T6yvAb4AF7vu5wE3geS51wxPNVETCgLaqOg/nIUSNgeNqK8ZUFTsiMeYX0SKyymv4c1UtuSy1rogsxjmQGueO+wPwhoj8GedJbNe5428DXhGR63FqBDfh9H7pSzjwXxFphNOL59PqPNrTmKCwcwrGlMM9p9BPVfcFOxZjAs2aj4wxxnhYTcEYY4yH1RSMMcZ4WFIwxhjjYUnBGGOMhyUFY4wxHpYUjDHGePx/qn/TyHSI/BUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n",
      "1500/1500 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3132451387405395, 0.7228000000317891]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3178732957839965, 0.7146666671435038]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/feraguilari/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.9731 - acc: 0.1525 - val_loss: 1.9291 - val_acc: 0.1760\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9478 - acc: 0.1720 - val_loss: 1.9147 - val_acc: 0.2020\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9389 - acc: 0.1849 - val_loss: 1.9035 - val_acc: 0.2190\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9227 - acc: 0.1957 - val_loss: 1.8914 - val_acc: 0.2240\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9152 - acc: 0.1964 - val_loss: 1.8809 - val_acc: 0.2410\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9036 - acc: 0.2008 - val_loss: 1.8683 - val_acc: 0.2420\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8949 - acc: 0.2213 - val_loss: 1.8561 - val_acc: 0.2490\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8785 - acc: 0.2241 - val_loss: 1.8419 - val_acc: 0.2640\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8737 - acc: 0.2289 - val_loss: 1.8277 - val_acc: 0.2750\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8538 - acc: 0.2468 - val_loss: 1.8115 - val_acc: 0.2940\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8482 - acc: 0.2540 - val_loss: 1.7971 - val_acc: 0.3100\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8294 - acc: 0.2676 - val_loss: 1.7775 - val_acc: 0.3250\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8138 - acc: 0.2707 - val_loss: 1.7574 - val_acc: 0.3550\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7939 - acc: 0.2829 - val_loss: 1.7360 - val_acc: 0.3890\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7778 - acc: 0.2943 - val_loss: 1.7107 - val_acc: 0.3890\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7649 - acc: 0.3051 - val_loss: 1.6899 - val_acc: 0.4220\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7514 - acc: 0.3136 - val_loss: 1.6665 - val_acc: 0.4490\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7288 - acc: 0.3253 - val_loss: 1.6451 - val_acc: 0.4640\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7120 - acc: 0.3363 - val_loss: 1.6182 - val_acc: 0.4750\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6910 - acc: 0.3496 - val_loss: 1.5925 - val_acc: 0.4990\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6752 - acc: 0.3472 - val_loss: 1.5690 - val_acc: 0.5170\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6532 - acc: 0.3599 - val_loss: 1.5432 - val_acc: 0.5310\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6434 - acc: 0.3655 - val_loss: 1.5219 - val_acc: 0.5460\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6277 - acc: 0.3743 - val_loss: 1.4972 - val_acc: 0.5610\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6013 - acc: 0.3908 - val_loss: 1.4686 - val_acc: 0.5680\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5878 - acc: 0.3971 - val_loss: 1.4462 - val_acc: 0.5850\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5767 - acc: 0.3984 - val_loss: 1.4194 - val_acc: 0.5900\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5469 - acc: 0.4148 - val_loss: 1.3925 - val_acc: 0.6020\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5305 - acc: 0.4180 - val_loss: 1.3688 - val_acc: 0.6160\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5110 - acc: 0.4369 - val_loss: 1.3434 - val_acc: 0.6170\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4941 - acc: 0.4287 - val_loss: 1.3228 - val_acc: 0.6250\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4770 - acc: 0.4412 - val_loss: 1.2962 - val_acc: 0.6310\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4578 - acc: 0.4527 - val_loss: 1.2761 - val_acc: 0.6340\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4424 - acc: 0.4568 - val_loss: 1.2520 - val_acc: 0.6450\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4324 - acc: 0.4593 - val_loss: 1.2321 - val_acc: 0.6480\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4107 - acc: 0.4693 - val_loss: 1.2141 - val_acc: 0.6670\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3998 - acc: 0.4693 - val_loss: 1.1921 - val_acc: 0.6690\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3796 - acc: 0.4745 - val_loss: 1.1744 - val_acc: 0.6710\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3691 - acc: 0.4932 - val_loss: 1.1592 - val_acc: 0.6730\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3551 - acc: 0.4911 - val_loss: 1.1399 - val_acc: 0.6730\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3516 - acc: 0.4859 - val_loss: 1.1298 - val_acc: 0.6810\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3353 - acc: 0.5016 - val_loss: 1.1158 - val_acc: 0.6860\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3075 - acc: 0.5131 - val_loss: 1.0979 - val_acc: 0.6870\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3008 - acc: 0.5151 - val_loss: 1.0846 - val_acc: 0.6850\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2917 - acc: 0.5175 - val_loss: 1.0709 - val_acc: 0.6900\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2827 - acc: 0.5229 - val_loss: 1.0604 - val_acc: 0.6990\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2719 - acc: 0.5207 - val_loss: 1.0452 - val_acc: 0.7000\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2673 - acc: 0.5243 - val_loss: 1.0353 - val_acc: 0.7020\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2509 - acc: 0.5296 - val_loss: 1.0210 - val_acc: 0.7060\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2458 - acc: 0.5345 - val_loss: 1.0108 - val_acc: 0.7060\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2286 - acc: 0.5428 - val_loss: 0.9992 - val_acc: 0.7040\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2166 - acc: 0.5413 - val_loss: 0.9889 - val_acc: 0.7120\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1995 - acc: 0.5511 - val_loss: 0.9761 - val_acc: 0.7090\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2015 - acc: 0.5483 - val_loss: 0.9681 - val_acc: 0.7160\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1978 - acc: 0.5540 - val_loss: 0.9593 - val_acc: 0.7170\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1914 - acc: 0.5551 - val_loss: 0.9512 - val_acc: 0.7150\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1813 - acc: 0.5616 - val_loss: 0.9420 - val_acc: 0.7140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1748 - acc: 0.5609 - val_loss: 0.9353 - val_acc: 0.7200\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1624 - acc: 0.5655 - val_loss: 0.9277 - val_acc: 0.7220\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1612 - acc: 0.5595 - val_loss: 0.9203 - val_acc: 0.7200\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1499 - acc: 0.5752 - val_loss: 0.9120 - val_acc: 0.7240\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1394 - acc: 0.5729 - val_loss: 0.9050 - val_acc: 0.7250\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1205 - acc: 0.5800 - val_loss: 0.8981 - val_acc: 0.7210\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1312 - acc: 0.5699 - val_loss: 0.8924 - val_acc: 0.7240\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1067 - acc: 0.5967 - val_loss: 0.8817 - val_acc: 0.7260\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1078 - acc: 0.5903 - val_loss: 0.8742 - val_acc: 0.7270\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1037 - acc: 0.5891 - val_loss: 0.8710 - val_acc: 0.7310\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0965 - acc: 0.5977 - val_loss: 0.8626 - val_acc: 0.7280\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0972 - acc: 0.5979 - val_loss: 0.8584 - val_acc: 0.7280\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0919 - acc: 0.5937 - val_loss: 0.8550 - val_acc: 0.7330\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0857 - acc: 0.6009 - val_loss: 0.8510 - val_acc: 0.7370\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0696 - acc: 0.6063 - val_loss: 0.8432 - val_acc: 0.7360\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0683 - acc: 0.6033 - val_loss: 0.8388 - val_acc: 0.7350\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0575 - acc: 0.6047 - val_loss: 0.8318 - val_acc: 0.7350\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0673 - acc: 0.5977 - val_loss: 0.8268 - val_acc: 0.7320\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0596 - acc: 0.6115 - val_loss: 0.8252 - val_acc: 0.7330\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0486 - acc: 0.6152 - val_loss: 0.8215 - val_acc: 0.7330\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0363 - acc: 0.6183 - val_loss: 0.8145 - val_acc: 0.7360\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0419 - acc: 0.6156 - val_loss: 0.8117 - val_acc: 0.7370\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0268 - acc: 0.6213 - val_loss: 0.8043 - val_acc: 0.7330\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0262 - acc: 0.6195 - val_loss: 0.8019 - val_acc: 0.7380\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0299 - acc: 0.6213 - val_loss: 0.7979 - val_acc: 0.7420\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0171 - acc: 0.6267 - val_loss: 0.7959 - val_acc: 0.7400\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0181 - acc: 0.6187 - val_loss: 0.7905 - val_acc: 0.7430\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9980 - acc: 0.6325 - val_loss: 0.7823 - val_acc: 0.7420\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0167 - acc: 0.6231 - val_loss: 0.7821 - val_acc: 0.7450\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0028 - acc: 0.6297 - val_loss: 0.7767 - val_acc: 0.7450\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0038 - acc: 0.6252 - val_loss: 0.7751 - val_acc: 0.7430\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9911 - acc: 0.6321 - val_loss: 0.7712 - val_acc: 0.7470\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9899 - acc: 0.6376 - val_loss: 0.7675 - val_acc: 0.7460\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9868 - acc: 0.6385 - val_loss: 0.7639 - val_acc: 0.7490\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9966 - acc: 0.6332 - val_loss: 0.7613 - val_acc: 0.7470\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9704 - acc: 0.6469 - val_loss: 0.7563 - val_acc: 0.7460\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9590 - acc: 0.6425 - val_loss: 0.7527 - val_acc: 0.7470\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9707 - acc: 0.6347 - val_loss: 0.7486 - val_acc: 0.7520\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9593 - acc: 0.6459 - val_loss: 0.7465 - val_acc: 0.7450\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9677 - acc: 0.6401 - val_loss: 0.7429 - val_acc: 0.7480\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9352 - acc: 0.6543 - val_loss: 0.7398 - val_acc: 0.7510\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9400 - acc: 0.6531 - val_loss: 0.7360 - val_acc: 0.7530\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9382 - acc: 0.6591 - val_loss: 0.7322 - val_acc: 0.7510\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9499 - acc: 0.6536 - val_loss: 0.7316 - val_acc: 0.7490\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9415 - acc: 0.6560 - val_loss: 0.7294 - val_acc: 0.7510\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9252 - acc: 0.6624 - val_loss: 0.7264 - val_acc: 0.7520\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9310 - acc: 0.6600 - val_loss: 0.7246 - val_acc: 0.7540\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9255 - acc: 0.6593 - val_loss: 0.7226 - val_acc: 0.7540\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9166 - acc: 0.6692 - val_loss: 0.7214 - val_acc: 0.7510\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9252 - acc: 0.6617 - val_loss: 0.7150 - val_acc: 0.7510\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9172 - acc: 0.6637 - val_loss: 0.7145 - val_acc: 0.7560\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9128 - acc: 0.6581 - val_loss: 0.7134 - val_acc: 0.7520\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9053 - acc: 0.6740 - val_loss: 0.7110 - val_acc: 0.7550\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9077 - acc: 0.6708 - val_loss: 0.7084 - val_acc: 0.7590\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9110 - acc: 0.6673 - val_loss: 0.7069 - val_acc: 0.7580\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9237 - acc: 0.6637 - val_loss: 0.7069 - val_acc: 0.7570\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9096 - acc: 0.6684 - val_loss: 0.7049 - val_acc: 0.7570\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9019 - acc: 0.6657 - val_loss: 0.7014 - val_acc: 0.7590\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8891 - acc: 0.6707 - val_loss: 0.6996 - val_acc: 0.7570\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8942 - acc: 0.6724 - val_loss: 0.6971 - val_acc: 0.7580\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8812 - acc: 0.6777 - val_loss: 0.6930 - val_acc: 0.7570\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8773 - acc: 0.6761 - val_loss: 0.6904 - val_acc: 0.7610\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8705 - acc: 0.6808 - val_loss: 0.6906 - val_acc: 0.7620\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8674 - acc: 0.6840 - val_loss: 0.6870 - val_acc: 0.7640\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8613 - acc: 0.6901 - val_loss: 0.6829 - val_acc: 0.7630\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8664 - acc: 0.6821 - val_loss: 0.6820 - val_acc: 0.7610\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8578 - acc: 0.6900 - val_loss: 0.6814 - val_acc: 0.7620\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8666 - acc: 0.6817 - val_loss: 0.6818 - val_acc: 0.7590\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8573 - acc: 0.6865 - val_loss: 0.6812 - val_acc: 0.7640\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8650 - acc: 0.6857 - val_loss: 0.6771 - val_acc: 0.7640\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8574 - acc: 0.6900 - val_loss: 0.6769 - val_acc: 0.7670\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8625 - acc: 0.6797 - val_loss: 0.6746 - val_acc: 0.7620\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8417 - acc: 0.6900 - val_loss: 0.6731 - val_acc: 0.7590\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8613 - acc: 0.6876 - val_loss: 0.6736 - val_acc: 0.7620\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8477 - acc: 0.6939 - val_loss: 0.6701 - val_acc: 0.7680\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8448 - acc: 0.6921 - val_loss: 0.6677 - val_acc: 0.7670\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8442 - acc: 0.6957 - val_loss: 0.6683 - val_acc: 0.7630\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8494 - acc: 0.6915 - val_loss: 0.6664 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8383 - acc: 0.6952 - val_loss: 0.6655 - val_acc: 0.7680\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8459 - acc: 0.6919 - val_loss: 0.6653 - val_acc: 0.7690\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8465 - acc: 0.6928 - val_loss: 0.6629 - val_acc: 0.7680\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8192 - acc: 0.7080 - val_loss: 0.6597 - val_acc: 0.7720\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8198 - acc: 0.6955 - val_loss: 0.6584 - val_acc: 0.7690\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8265 - acc: 0.6959 - val_loss: 0.6566 - val_acc: 0.7680\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8340 - acc: 0.6891 - val_loss: 0.6576 - val_acc: 0.7710\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8102 - acc: 0.7033 - val_loss: 0.6557 - val_acc: 0.7650\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8225 - acc: 0.7011 - val_loss: 0.6530 - val_acc: 0.7710\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8154 - acc: 0.7039 - val_loss: 0.6514 - val_acc: 0.7690\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8218 - acc: 0.6960 - val_loss: 0.6513 - val_acc: 0.7710\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8251 - acc: 0.6933 - val_loss: 0.6513 - val_acc: 0.7700\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8087 - acc: 0.6972 - val_loss: 0.6487 - val_acc: 0.7690\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8131 - acc: 0.7036 - val_loss: 0.6512 - val_acc: 0.7680\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8204 - acc: 0.6963 - val_loss: 0.6470 - val_acc: 0.7750\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8085 - acc: 0.7047 - val_loss: 0.6471 - val_acc: 0.7730\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8143 - acc: 0.7087 - val_loss: 0.6472 - val_acc: 0.7680\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7973 - acc: 0.7085 - val_loss: 0.6435 - val_acc: 0.7730\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7974 - acc: 0.6991 - val_loss: 0.6441 - val_acc: 0.7730\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7988 - acc: 0.7055 - val_loss: 0.6441 - val_acc: 0.7710\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7923 - acc: 0.7137 - val_loss: 0.6406 - val_acc: 0.7730\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7888 - acc: 0.7081 - val_loss: 0.6394 - val_acc: 0.7730\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7996 - acc: 0.7077 - val_loss: 0.6397 - val_acc: 0.7730\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8090 - acc: 0.7099 - val_loss: 0.6403 - val_acc: 0.7740\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7874 - acc: 0.7128 - val_loss: 0.6382 - val_acc: 0.7770\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7793 - acc: 0.7216 - val_loss: 0.6367 - val_acc: 0.7740\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7817 - acc: 0.7135 - val_loss: 0.6369 - val_acc: 0.7740\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7875 - acc: 0.7165 - val_loss: 0.6335 - val_acc: 0.7730\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7823 - acc: 0.7141 - val_loss: 0.6345 - val_acc: 0.7760\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7731 - acc: 0.7213 - val_loss: 0.6343 - val_acc: 0.7770\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7753 - acc: 0.7159 - val_loss: 0.6339 - val_acc: 0.7780\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7717 - acc: 0.7196 - val_loss: 0.6327 - val_acc: 0.7750\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7738 - acc: 0.7141 - val_loss: 0.6307 - val_acc: 0.7790\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7621 - acc: 0.7253 - val_loss: 0.6280 - val_acc: 0.7760\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7603 - acc: 0.7215 - val_loss: 0.6274 - val_acc: 0.7790\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7632 - acc: 0.7257 - val_loss: 0.6269 - val_acc: 0.7800\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7725 - acc: 0.7276 - val_loss: 0.6267 - val_acc: 0.7790\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7559 - acc: 0.7245 - val_loss: 0.6241 - val_acc: 0.7820\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7652 - acc: 0.7185 - val_loss: 0.6240 - val_acc: 0.7790\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7518 - acc: 0.7240 - val_loss: 0.6236 - val_acc: 0.7800\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7595 - acc: 0.7184 - val_loss: 0.6259 - val_acc: 0.7830\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7651 - acc: 0.7277 - val_loss: 0.6232 - val_acc: 0.7830\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7512 - acc: 0.7252 - val_loss: 0.6210 - val_acc: 0.7820\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7539 - acc: 0.7221 - val_loss: 0.6203 - val_acc: 0.7800\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7525 - acc: 0.7211 - val_loss: 0.6195 - val_acc: 0.7830\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7521 - acc: 0.7188 - val_loss: 0.6172 - val_acc: 0.7840\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7602 - acc: 0.7229 - val_loss: 0.6190 - val_acc: 0.7810\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7544 - acc: 0.7231 - val_loss: 0.6170 - val_acc: 0.7830\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7459 - acc: 0.7296 - val_loss: 0.6166 - val_acc: 0.7810\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7242 - acc: 0.7324 - val_loss: 0.6149 - val_acc: 0.7790\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7489 - acc: 0.7264 - val_loss: 0.6141 - val_acc: 0.7770\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7496 - acc: 0.7292 - val_loss: 0.6158 - val_acc: 0.7780\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7408 - acc: 0.7301 - val_loss: 0.6158 - val_acc: 0.7790\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7372 - acc: 0.7308 - val_loss: 0.6141 - val_acc: 0.7840\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7428 - acc: 0.7259 - val_loss: 0.6158 - val_acc: 0.7830\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7403 - acc: 0.7323 - val_loss: 0.6149 - val_acc: 0.7790\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7321 - acc: 0.7371 - val_loss: 0.6126 - val_acc: 0.7840\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7289 - acc: 0.7311 - val_loss: 0.6102 - val_acc: 0.7850\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7295 - acc: 0.7312 - val_loss: 0.6110 - val_acc: 0.7830\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7259 - acc: 0.7304 - val_loss: 0.6107 - val_acc: 0.7830\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7344 - acc: 0.7340 - val_loss: 0.6078 - val_acc: 0.7920\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7214 - acc: 0.7349 - val_loss: 0.6099 - val_acc: 0.7850\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7257 - acc: 0.7373 - val_loss: 0.6100 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7080 - acc: 0.7356 - val_loss: 0.6072 - val_acc: 0.7880\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7221 - acc: 0.7372 - val_loss: 0.6078 - val_acc: 0.7860\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44953240927060445, 0.8355999999682109]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6567809325853984, 0.745333333492279]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9059 - acc: 0.2357 - val_loss: 1.8447 - val_acc: 0.3040\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.7693 - acc: 0.3337 - val_loss: 1.6803 - val_acc: 0.3730\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.5832 - acc: 0.4158 - val_loss: 1.4830 - val_acc: 0.4650\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.3903 - acc: 0.5098 - val_loss: 1.3003 - val_acc: 0.5380\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.2231 - acc: 0.5806 - val_loss: 1.1496 - val_acc: 0.6143\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.0873 - acc: 0.6368 - val_loss: 1.0279 - val_acc: 0.6507\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.9767 - acc: 0.6762 - val_loss: 0.9305 - val_acc: 0.6863\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8884 - acc: 0.7037 - val_loss: 0.8548 - val_acc: 0.7063\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8198 - acc: 0.7216 - val_loss: 0.7978 - val_acc: 0.7183\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7662 - acc: 0.7361 - val_loss: 0.7528 - val_acc: 0.7357\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7247 - acc: 0.7462 - val_loss: 0.7190 - val_acc: 0.7413\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6919 - acc: 0.7554 - val_loss: 0.6924 - val_acc: 0.7480\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.6653 - acc: 0.7634 - val_loss: 0.6719 - val_acc: 0.7550\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6434 - acc: 0.7690 - val_loss: 0.6556 - val_acc: 0.7570\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6252 - acc: 0.7755 - val_loss: 0.6425 - val_acc: 0.7637\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6093 - acc: 0.7800 - val_loss: 0.6335 - val_acc: 0.7673\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5955 - acc: 0.7857 - val_loss: 0.6248 - val_acc: 0.7710\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5831 - acc: 0.7889 - val_loss: 0.6167 - val_acc: 0.7727\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5721 - acc: 0.7927 - val_loss: 0.6092 - val_acc: 0.7750\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5619 - acc: 0.7968 - val_loss: 0.6005 - val_acc: 0.7790\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5524 - acc: 0.7997 - val_loss: 0.5982 - val_acc: 0.7797\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5440 - acc: 0.8037 - val_loss: 0.5905 - val_acc: 0.7833\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5360 - acc: 0.8058 - val_loss: 0.5867 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5287 - acc: 0.8095 - val_loss: 0.5824 - val_acc: 0.7883\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5217 - acc: 0.8125 - val_loss: 0.5796 - val_acc: 0.7893\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5149 - acc: 0.8136 - val_loss: 0.5758 - val_acc: 0.7917\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5085 - acc: 0.8165 - val_loss: 0.5752 - val_acc: 0.7910\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5028 - acc: 0.8182 - val_loss: 0.5699 - val_acc: 0.7937\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4970 - acc: 0.8214 - val_loss: 0.5676 - val_acc: 0.7943\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4914 - acc: 0.8233 - val_loss: 0.5681 - val_acc: 0.7927\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4864 - acc: 0.8257 - val_loss: 0.5630 - val_acc: 0.7950\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.4812 - acc: 0.8270 - val_loss: 0.5615 - val_acc: 0.7963\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4767 - acc: 0.8288 - val_loss: 0.5626 - val_acc: 0.7987\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4719 - acc: 0.8305 - val_loss: 0.5600 - val_acc: 0.7987\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4676 - acc: 0.8319 - val_loss: 0.5583 - val_acc: 0.7993\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4633 - acc: 0.8337 - val_loss: 0.5618 - val_acc: 0.7973\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4592 - acc: 0.8355 - val_loss: 0.5552 - val_acc: 0.8020\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4552 - acc: 0.8372 - val_loss: 0.5561 - val_acc: 0.7990\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4511 - acc: 0.8395 - val_loss: 0.5541 - val_acc: 0.8007\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4475 - acc: 0.8399 - val_loss: 0.5531 - val_acc: 0.8023\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4438 - acc: 0.8421 - val_loss: 0.5518 - val_acc: 0.8003\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4403 - acc: 0.8436 - val_loss: 0.5537 - val_acc: 0.8013\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4366 - acc: 0.8444 - val_loss: 0.5513 - val_acc: 0.8020\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4338 - acc: 0.8466 - val_loss: 0.5498 - val_acc: 0.8013\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4303 - acc: 0.8482 - val_loss: 0.5525 - val_acc: 0.8033\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4272 - acc: 0.8489 - val_loss: 0.5490 - val_acc: 0.8033\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4240 - acc: 0.8506 - val_loss: 0.5516 - val_acc: 0.8020\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4208 - acc: 0.8515 - val_loss: 0.5489 - val_acc: 0.8020\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4180 - acc: 0.8526 - val_loss: 0.5509 - val_acc: 0.8047\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4157 - acc: 0.8535 - val_loss: 0.5477 - val_acc: 0.7997\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4127 - acc: 0.8550 - val_loss: 0.5505 - val_acc: 0.8007\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4096 - acc: 0.8561 - val_loss: 0.5515 - val_acc: 0.8037\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4070 - acc: 0.8559 - val_loss: 0.5489 - val_acc: 0.8027\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4044 - acc: 0.8585 - val_loss: 0.5506 - val_acc: 0.8020\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4015 - acc: 0.8591 - val_loss: 0.5506 - val_acc: 0.8017\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3991 - acc: 0.8598 - val_loss: 0.5501 - val_acc: 0.8023\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3969 - acc: 0.8601 - val_loss: 0.5498 - val_acc: 0.7987\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3946 - acc: 0.8614 - val_loss: 0.5513 - val_acc: 0.7993\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3923 - acc: 0.8628 - val_loss: 0.5506 - val_acc: 0.8013\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3898 - acc: 0.8640 - val_loss: 0.5512 - val_acc: 0.8020\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3874 - acc: 0.8649 - val_loss: 0.5505 - val_acc: 0.8033\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3855 - acc: 0.8654 - val_loss: 0.5541 - val_acc: 0.8023\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3828 - acc: 0.8664 - val_loss: 0.5560 - val_acc: 0.8003\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3808 - acc: 0.8672 - val_loss: 0.5517 - val_acc: 0.8010\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3786 - acc: 0.8678 - val_loss: 0.5524 - val_acc: 0.8030\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3769 - acc: 0.8694 - val_loss: 0.5556 - val_acc: 0.8003\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3745 - acc: 0.8698 - val_loss: 0.5539 - val_acc: 0.8013\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3725 - acc: 0.8706 - val_loss: 0.5525 - val_acc: 0.7993\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3706 - acc: 0.8710 - val_loss: 0.5552 - val_acc: 0.8013\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3686 - acc: 0.8718 - val_loss: 0.5536 - val_acc: 0.8033\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3667 - acc: 0.8729 - val_loss: 0.5578 - val_acc: 0.8000\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3648 - acc: 0.8740 - val_loss: 0.5547 - val_acc: 0.8000\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3632 - acc: 0.8730 - val_loss: 0.5577 - val_acc: 0.8010\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3610 - acc: 0.8746 - val_loss: 0.5593 - val_acc: 0.7990\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3590 - acc: 0.8756 - val_loss: 0.5614 - val_acc: 0.7980\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3574 - acc: 0.8760 - val_loss: 0.5573 - val_acc: 0.8007\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3558 - acc: 0.8773 - val_loss: 0.5639 - val_acc: 0.7987\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3542 - acc: 0.8768 - val_loss: 0.5607 - val_acc: 0.7977\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3518 - acc: 0.8783 - val_loss: 0.5626 - val_acc: 0.7977\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3506 - acc: 0.8785 - val_loss: 0.5651 - val_acc: 0.7993\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3485 - acc: 0.8792 - val_loss: 0.5616 - val_acc: 0.7983\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3475 - acc: 0.8801 - val_loss: 0.5739 - val_acc: 0.8000\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3454 - acc: 0.8809 - val_loss: 0.5669 - val_acc: 0.7963\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3437 - acc: 0.8809 - val_loss: 0.5660 - val_acc: 0.7987\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3421 - acc: 0.8816 - val_loss: 0.5654 - val_acc: 0.7980\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3406 - acc: 0.8826 - val_loss: 0.5663 - val_acc: 0.7967\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3388 - acc: 0.8822 - val_loss: 0.5685 - val_acc: 0.7987\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3375 - acc: 0.8829 - val_loss: 0.5683 - val_acc: 0.7977\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3360 - acc: 0.8842 - val_loss: 0.5703 - val_acc: 0.7963\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3343 - acc: 0.8850 - val_loss: 0.5699 - val_acc: 0.7967\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3330 - acc: 0.8851 - val_loss: 0.5726 - val_acc: 0.7957\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3314 - acc: 0.8849 - val_loss: 0.5722 - val_acc: 0.7963\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3301 - acc: 0.8855 - val_loss: 0.5784 - val_acc: 0.7997\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3285 - acc: 0.8862 - val_loss: 0.5734 - val_acc: 0.7960\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3270 - acc: 0.8875 - val_loss: 0.5781 - val_acc: 0.7983\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3257 - acc: 0.8880 - val_loss: 0.5753 - val_acc: 0.7973\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3239 - acc: 0.8884 - val_loss: 0.5823 - val_acc: 0.7987\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3226 - acc: 0.8892 - val_loss: 0.5799 - val_acc: 0.7967\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3210 - acc: 0.8891 - val_loss: 0.5814 - val_acc: 0.7987\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3195 - acc: 0.8905 - val_loss: 0.5801 - val_acc: 0.7990\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3186 - acc: 0.8906 - val_loss: 0.5817 - val_acc: 0.7983\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3172 - acc: 0.8900 - val_loss: 0.5806 - val_acc: 0.7973\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3158 - acc: 0.8908 - val_loss: 0.5827 - val_acc: 0.7970\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3144 - acc: 0.8919 - val_loss: 0.5830 - val_acc: 0.7973\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3129 - acc: 0.8918 - val_loss: 0.5870 - val_acc: 0.7970\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3120 - acc: 0.8931 - val_loss: 0.5878 - val_acc: 0.7977\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3102 - acc: 0.8935 - val_loss: 0.5862 - val_acc: 0.7990\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3092 - acc: 0.8931 - val_loss: 0.5878 - val_acc: 0.7970\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3079 - acc: 0.8940 - val_loss: 0.5899 - val_acc: 0.7990\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3063 - acc: 0.8951 - val_loss: 0.5941 - val_acc: 0.7963\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8962 - val_loss: 0.5979 - val_acc: 0.7980\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8962 - val_loss: 0.5910 - val_acc: 0.7983\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8963 - val_loss: 0.5982 - val_acc: 0.7960\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3009 - acc: 0.8970 - val_loss: 0.5915 - val_acc: 0.7957\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3005 - acc: 0.8976 - val_loss: 0.5956 - val_acc: 0.7990\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.8978 - val_loss: 0.5984 - val_acc: 0.7977\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2976 - acc: 0.8994 - val_loss: 0.5987 - val_acc: 0.7970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2965 - acc: 0.8987 - val_loss: 0.6030 - val_acc: 0.7970\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.8994 - val_loss: 0.6058 - val_acc: 0.7973\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2941 - acc: 0.9004 - val_loss: 0.6004 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 20us/step\n",
      "4000/4000 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2879502433448127, 0.9040909090909091]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5978418927192688, 0.80125]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
